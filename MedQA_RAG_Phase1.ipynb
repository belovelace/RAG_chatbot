{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/belovelace/RAG_chatbot/blob/main/MedQA_RAG_Phase1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ğŸ¥ MedQA ê¸°ë°˜ ì˜ë£Œ RAG ì‹œìŠ¤í…œ - Phase 1\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ MedQA ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ì˜ë£Œ íŠ¹í™” RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
        "\n",
        "## Phase 1 ëª©í‘œ\n",
        "1. MedQA ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\n",
        "2. ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ì— MedQA ë°ì´í„° ì ìš©\n",
        "3. ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°œë°œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ê°€ì ¸ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975eb3ed-0f99-4879-9ded-c68af6ae95df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install openai tiktoken python-dotenv scipy pandas numpy requests\n",
        "!pip install datasets  # Hugging Face datasets for MedQA\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "from scipy import spatial\n",
        "from dotenv import load_dotenv\n",
        "from datasets import load_dataset\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 2. OpenAI API ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_api",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04999a1-9197-4e32-94fb-13e8dc35e200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OpenAI API ì„¤ì • ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)\n",
        "# ë³´ì•ˆì„ ìœ„í•´ getpass ì‚¬ìš©\n",
        "import getpass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    api_key = getpass.getpass(\"OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "embedding_encoding = \"cl100k_base\"\n",
        "max_tokens = 1500\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(embedding_encoding)\n",
        "print(\"âœ… OpenAI API ì„¤ì • ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medqa_load"
      },
      "source": [
        "## 3. MedQA ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"train\")"
      ],
      "metadata": {
        "id": "E--9dhgAkqCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_medqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e8b760-5e77-4886-8932-65d3f02fc804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'bigbio/med_qa' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'bigbio/med_qa' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ MedQA ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘ (ë°©ë²• 1: trust_remote_code)...\n",
            "âŒ ë°©ë²• 1 ì‹¤íŒ¨: Dataset scripts are no longer supported, but found med_qa.py\n",
            "ğŸ”„ ì‹œë„ ì¤‘: GBaker/MedQA-USMLE-4-options\n",
            "âœ… GBaker/MedQA-USMLE-4-options ë¡œë”© ì„±ê³µ: 500ê°œ\n",
            "âœ… GBaker/MedQA-USMLE-4-options ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ 500ê°œ í•­ëª©\n",
            "\n",
            "ğŸ“Š ë°ì´í„° ìƒ˜í”Œ:\n",
            "                title                                           question  \\\n",
            "0  Medical Question 1  A 23-year-old pregnant woman at 22 weeks gesta...   \n",
            "1  Medical Question 2  A 3-month-old baby died suddenly at night whil...   \n",
            "2  Medical Question 3  A mother brings her 3-week-old infant to the p...   \n",
            "3  Medical Question 4  A pulmonary autopsy specimen from a 58-year-ol...   \n",
            "4  Medical Question 5  A 20-year-old woman presents with menorrhagia ...   \n",
            "\n",
            "                                      correct_answer  \n",
            "0                                         Ampicillin  \n",
            "1  Placing the infant in a supine position on a f...  \n",
            "2       Abnormal migration of ventral pancreatic bud  \n",
            "3                                    Thromboembolism  \n",
            "4                                       Hemophilia A  \n",
            "\n",
            "ğŸ“ˆ ë°ì´í„°ì…‹ ì •ë³´:\n",
            "- ì´ í•­ëª© ìˆ˜: 500\n",
            "- ë°ì´í„° ì†ŒìŠ¤: {'GBaker/MedQA-USMLE-4-options': 500}\n",
            "- ì¹´í…Œê³ ë¦¬: {'Medical Knowledge': 500}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "def load_medqa_dataset(subset_size: int = 1000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    MedQA ë°ì´í„°ì…‹ì„ ë¡œë”©í•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "    ì—¬ëŸ¬ ë°©ë²•ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ì—¬ ìµœëŒ€í•œ ì‹¤ì œ ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.\n",
        "\n",
        "    Args:\n",
        "        subset_size: ì‚¬ìš©í•  ë°ì´í„° ê°œìˆ˜ (ì „ì²´ ë°ì´í„°ì…‹ì´ í¬ë¯€ë¡œ ì¼ë¶€ë§Œ ì‚¬ìš©)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: ì „ì²˜ë¦¬ëœ MedQA ë°ì´í„°\n",
        "    \"\"\"\n",
        "\n",
        "    # ë°©ë²• 1: trust_remote_code=Trueë¡œ ì‹œë„\n",
        "    try:\n",
        "        print(\"ğŸ”„ MedQA ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘ (ë°©ë²• 1: trust_remote_code)...\")\n",
        "        dataset = load_dataset(\"bigbio/med_qa\", \"med_qa_en_bigbio_qa\",\n",
        "                             split=\"train\", trust_remote_code=True)\n",
        "        return process_medqa_dataset(dataset, subset_size)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ë°©ë²• 1 ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    # ë°©ë²• 2: ë‹¤ë¥¸ MedQA ë³€í˜• ì‹œë„\n",
        "    medqa_variants = [\n",
        "        (\"GBaker/MedQA-USMLE-4-options\", None),\n",
        "        (\"medmcqa\", None),\n",
        "        (\"pubmed_qa\", \"pqa_labeled\"),\n",
        "        (\"bigbio/med_qa\", \"med_qa_en_source\")\n",
        "    ]\n",
        "\n",
        "    for dataset_name, config in medqa_variants:\n",
        "        try:\n",
        "            print(f\"ğŸ”„ ì‹œë„ ì¤‘: {dataset_name}\")\n",
        "            if config:\n",
        "                dataset = load_dataset(dataset_name, config, split=\"train\")\n",
        "            else:\n",
        "                dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "            return process_alternative_dataset(dataset, dataset_name, subset_size)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {dataset_name} ì‹¤íŒ¨: {e}\")\n",
        "            continue\n",
        "\n",
        "    # ë°©ë²• 3: ë¡œì»¬ JSONL íŒŒì¼ ì§ì ‘ ë¡œë”© ì‹œë„\n",
        "    print(\"ğŸ”„ ë¡œì»¬ JSONL íŒŒì¼ í™•ì¸ ì¤‘...\")\n",
        "    local_files = [\n",
        "        \"phrases_no_exclude_train.jsonl\",\n",
        "        \"phrases_no_exclude_test.jsonl\",\n",
        "        \"./phrases_no_exclude_train.jsonl\",\n",
        "        \"./phrases_no_exclude_test.jsonl\"\n",
        "    ]\n",
        "\n",
        "    for file_path in local_files:\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                print(f\"ğŸ“ ë¡œì»¬ íŒŒì¼ ë°œê²¬: {file_path}\")\n",
        "                return load_from_jsonl(file_path, subset_size)\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
        "                continue\n",
        "\n",
        "    # ë°©ë²• 4: ì˜¨ë¼ì¸ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„\n",
        "    print(\"ğŸ”„ ì˜¨ë¼ì¸ ì†ŒìŠ¤ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„...\")\n",
        "    try:\n",
        "        return download_medqa_directly(subset_size)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    # ìµœì¢…: ëª¨ë“  ë°©ë²• ì‹¤íŒ¨\n",
        "    raise Exception(\"âŒ ëª¨ë“  MedQA ë°ì´í„° ë¡œë”© ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ë‚˜ ë°ì´í„°ì…‹ ê°€ìš©ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "\n",
        "def process_medqa_dataset(dataset, subset_size: int) -> pd.DataFrame:\n",
        "    \"\"\"ì›ë³¸ MedQA ë°ì´í„°ì…‹ ì²˜ë¦¬\"\"\"\n",
        "    if len(dataset) > subset_size:\n",
        "        dataset = dataset.select(range(subset_size))\n",
        "\n",
        "    print(f\"âœ… ë¡œë”©ëœ ë°ì´í„° ê°œìˆ˜: {len(dataset)}\")\n",
        "\n",
        "    data_list = []\n",
        "    for idx, item in enumerate(dataset):\n",
        "        try:\n",
        "            question_text = item.get('question', '')\n",
        "            choices = item.get('choices', [])\n",
        "            answer_idx = item.get('answer', [0])[0] if item.get('answer') else 0\n",
        "\n",
        "            choices_text = \"\"\n",
        "            if choices:\n",
        "                choices_text = \"\\n\".join([f\"{chr(65+i)}) {choice}\" for i, choice in enumerate(choices)])\n",
        "\n",
        "            correct_answer = \"\"\n",
        "            if choices and answer_idx < len(choices):\n",
        "                correct_answer = choices[answer_idx]\n",
        "\n",
        "            full_text = f\"Question: {question_text}\\n\\nChoices:\\n{choices_text}\\n\\nCorrect Answer: {correct_answer}\"\n",
        "            document_id = item.get('document_id', f'medqa_{idx}')\n",
        "\n",
        "            data_list.append({\n",
        "                'id': document_id,\n",
        "                'title': f\"Medical Question {idx+1}\",\n",
        "                'question': question_text,\n",
        "                'choices': choices_text,\n",
        "                'correct_answer': correct_answer,\n",
        "                'full_text': full_text,\n",
        "                'source': 'MedQA',\n",
        "                'category': 'Medical Knowledge'\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {e}\")\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    print(f\"âœ… MedQA ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(df)}ê°œ í•­ëª©\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def process_alternative_dataset(dataset, dataset_name: str, subset_size: int) -> pd.DataFrame:\n",
        "    \"\"\"ëŒ€ì•ˆ ë°ì´í„°ì…‹ ì²˜ë¦¬\"\"\"\n",
        "    if len(dataset) > subset_size:\n",
        "        dataset = dataset.select(range(subset_size))\n",
        "\n",
        "    print(f\"âœ… {dataset_name} ë¡œë”© ì„±ê³µ: {len(dataset)}ê°œ\")\n",
        "\n",
        "    data_list = []\n",
        "    for idx, item in enumerate(dataset):\n",
        "        try:\n",
        "            # ë°ì´í„°ì…‹ë³„ êµ¬ì¡° ì²˜ë¦¬\n",
        "            if 'medmcqa' in dataset_name.lower():\n",
        "                question = str(item.get('question', ''))\n",
        "                choices = [str(item.get(f'op{chr(97+i)}', '')) for i in range(4)]  # opa, opb, opc, opd\n",
        "                answer_idx = item.get('cop', 0)\n",
        "\n",
        "            elif 'pubmed' in dataset_name.lower():\n",
        "                question = str(item.get('question', ''))\n",
        "                choices = ['Yes', 'No', 'Maybe']\n",
        "                answer_idx = item.get('final_decision', 0)\n",
        "\n",
        "            elif 'gbaker' in dataset_name.lower() or 'usmle' in dataset_name.lower():\n",
        "                # GBaker/MedQA-USMLE-4-options ì „ìš© ì²˜ë¦¬\n",
        "                question = str(item.get('question', ''))\n",
        "\n",
        "                # ì„ íƒì§€ ì²˜ë¦¬ - ë‹¤ì–‘í•œ í˜•íƒœ ì§€ì›\n",
        "                if 'options' in item:\n",
        "                    options = item['options']\n",
        "                    if isinstance(options, dict):\n",
        "                        choices = [str(options.get(key, '')) for key in sorted(options.keys())]\n",
        "                    elif isinstance(options, list):\n",
        "                        choices = [str(choice) for choice in options]\n",
        "                    else:\n",
        "                        choices = ['A', 'B', 'C', 'D']\n",
        "                elif 'choices' in item:\n",
        "                    choices_raw = item['choices']\n",
        "                    if isinstance(choices_raw, list):\n",
        "                        choices = [str(choice) for choice in choices_raw]\n",
        "                    else:\n",
        "                        choices = ['A', 'B', 'C', 'D']\n",
        "                else:\n",
        "                    # ê°œë³„ ì„ íƒì§€ í•„ë“œ í™•ì¸\n",
        "                    choices = []\n",
        "                    for i, option_key in enumerate(['A', 'B', 'C', 'D', 'option_a', 'option_b', 'option_c', 'option_d']):\n",
        "                        if option_key in item:\n",
        "                            choices.append(str(item[option_key]))\n",
        "\n",
        "                    if len(choices) == 0:\n",
        "                        choices = ['Option A', 'Option B', 'Option C', 'Option D']\n",
        "\n",
        "                # ì •ë‹µ ì¸ë±ìŠ¤ ì²˜ë¦¬\n",
        "                answer_idx = item.get('answer', item.get('correct_answer', item.get('answer_idx', 0)))\n",
        "\n",
        "            else:  # ê¸°íƒ€ í˜•ì‹\n",
        "                question = str(item.get('question', item.get('text', '')))\n",
        "                choices = item.get('choices', item.get('options', ['A', 'B', 'C', 'D']))\n",
        "                if isinstance(choices, str):\n",
        "                    choices = [choices]\n",
        "                elif not isinstance(choices, list):\n",
        "                    choices = ['A', 'B', 'C', 'D']\n",
        "                choices = [str(choice) for choice in choices]\n",
        "                answer_idx = item.get('answer', item.get('correct_answer', 0))\n",
        "\n",
        "            # answer_idx ì •ê·œí™” ë° íƒ€ì… ì²´í¬\n",
        "            if isinstance(answer_idx, list):\n",
        "                answer_idx = answer_idx[0] if answer_idx else 0\n",
        "\n",
        "            # ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ ì‹œë„\n",
        "            try:\n",
        "                if isinstance(answer_idx, str):\n",
        "                    if answer_idx.isdigit():\n",
        "                        answer_idx = int(answer_idx)\n",
        "                    elif answer_idx.upper() in ['A', 'B', 'C', 'D']:\n",
        "                        answer_idx = ord(answer_idx.upper()) - ord('A')\n",
        "                    else:\n",
        "                        answer_idx = 0\n",
        "                elif not isinstance(answer_idx, int):\n",
        "                    answer_idx = 0\n",
        "            except:\n",
        "                answer_idx = 0\n",
        "\n",
        "            # ì„ íƒì§€ í•„í„°ë§ (ë¹ˆ ì„ íƒì§€ ì œê±°)\n",
        "            choices = [choice.strip() for choice in choices if choice and str(choice).strip()]\n",
        "\n",
        "            # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬\n",
        "            if not question or not question.strip():\n",
        "                continue\n",
        "\n",
        "            if len(choices) == 0:\n",
        "                continue\n",
        "\n",
        "            # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì ‘ê·¼\n",
        "            if answer_idx >= len(choices) or answer_idx < 0:\n",
        "                answer_idx = 0\n",
        "\n",
        "            choices_text = \"\\n\".join([f\"{chr(65+i)}) {choice}\" for i, choice in enumerate(choices)])\n",
        "            correct_answer = choices[answer_idx] if choices else \"Unknown\"\n",
        "\n",
        "            full_text = f\"Question: {question}\\n\\nChoices:\\n{choices_text}\\n\\nCorrect Answer: {correct_answer}\"\n",
        "\n",
        "            data_list.append({\n",
        "                'id': f'{dataset_name.replace(\"/\", \"_\")}_{idx}',\n",
        "                'title': f\"Medical Question {idx+1}\",\n",
        "                'question': question,\n",
        "                'choices': choices_text,\n",
        "                'correct_answer': correct_answer,\n",
        "                'full_text': full_text,\n",
        "                'source': dataset_name,\n",
        "                'category': 'Medical Knowledge'\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {e}\")\n",
        "            # ë””ë²„ê¹…ìš© ìƒì„¸ ì •ë³´\n",
        "            print(f\"  - item keys: {list(item.keys()) if hasattr(item, 'keys') else 'No keys'}\")\n",
        "            if hasattr(item, 'keys') and 'answer' in item:\n",
        "                print(f\"  - answer value: {item['answer']} (type: {type(item['answer'])})\")\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    print(f\"âœ… {dataset_name} ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(df)}ê°œ í•­ëª©\")\n",
        "\n",
        "    # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ì²´í¬\n",
        "    if len(df) == 0:\n",
        "        print(f\"âš ï¸ {dataset_name}ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        # ì›ë³¸ ë°ì´í„° êµ¬ì¡° ë¶„ì„\n",
        "        if len(dataset) > 0:\n",
        "            sample_item = dataset[0]\n",
        "            print(f\"ğŸ“‹ ìƒ˜í”Œ ì•„ì´í…œ êµ¬ì¡°: {list(sample_item.keys())}\")\n",
        "            for key, value in sample_item.items():\n",
        "                print(f\"  - {key}: {type(value)} = {str(value)[:100]}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_from_jsonl(file_path: str, subset_size: int) -> pd.DataFrame:\n",
        "    \"\"\"JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë”©\"\"\"\n",
        "    data_list = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            if idx >= subset_size:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                item = json.loads(line.strip())\n",
        "\n",
        "                # JSONL êµ¬ì¡° íŒŒì‹± (êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
        "                question = item.get('question', item.get('text', ''))\n",
        "                choices = item.get('choices', item.get('options', []))\n",
        "                answer = item.get('answer', item.get('correct_answer', ''))\n",
        "\n",
        "                if isinstance(choices, list):\n",
        "                    choices_text = \"\\n\".join([f\"{chr(65+i)}) {choice}\" for i, choice in enumerate(choices)])\n",
        "                else:\n",
        "                    choices_text = str(choices)\n",
        "\n",
        "                full_text = f\"Question: {question}\\n\\nChoices:\\n{choices_text}\\n\\nCorrect Answer: {answer}\"\n",
        "\n",
        "                data_list.append({\n",
        "                    'id': f'local_{idx}',\n",
        "                    'title': f\"Medical Question {idx+1}\",\n",
        "                    'question': question,\n",
        "                    'choices': choices_text,\n",
        "                    'correct_answer': str(answer),\n",
        "                    'full_text': full_text,\n",
        "                    'source': 'Local JSONL',\n",
        "                    'category': 'Medical Knowledge'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ JSONL íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {idx}): {e}\")\n",
        "                continue\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    print(f\"âœ… JSONL íŒŒì¼ ë¡œë”© ì™„ë£Œ! ì´ {len(df)}ê°œ í•­ëª©\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def download_medqa_directly(subset_size: int) -> pd.DataFrame:\n",
        "    \"\"\"ì˜¨ë¼ì¸ì—ì„œ ì§ì ‘ MedQA ë°ì´í„° ë‹¤ìš´ë¡œë“œ\"\"\"\n",
        "    import urllib.request\n",
        "    import tempfile\n",
        "\n",
        "    # GitHubì´ë‚˜ ê³µê°œ ì €ì¥ì†Œì—ì„œ MedQA ìƒ˜í”Œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "    urls = [\n",
        "        \"https://raw.githubusercontent.com/jind11/MedQA/master/data/questions/4_options/questions_clean.json\",\n",
        "        \"https://raw.githubusercontent.com/jind11/MedQA/master/data/questions/5_options/questions_clean.json\"\n",
        "    ]\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            print(f\"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œë„: {url}\")\n",
        "\n",
        "            with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp_file:\n",
        "                urllib.request.urlretrieve(url, tmp_file.name)\n",
        "\n",
        "                with open(tmp_file.name, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                os.unlink(tmp_file.name)\n",
        "\n",
        "                # ë°ì´í„° ì²˜ë¦¬\n",
        "                data_list = []\n",
        "                for idx, item in enumerate(data[:subset_size]):\n",
        "                    question = item.get('question', '')\n",
        "                    options = item.get('options', {})\n",
        "                    answer_idx = item.get('answer_idx', 0)\n",
        "\n",
        "                    choices = [options.get(key, '') for key in sorted(options.keys())]\n",
        "                    choices_text = \"\\n\".join([f\"{chr(65+i)}) {choice}\" for i, choice in enumerate(choices)])\n",
        "                    correct_answer = choices[answer_idx] if answer_idx < len(choices) else \"Unknown\"\n",
        "\n",
        "                    full_text = f\"Question: {question}\\n\\nChoices:\\n{choices_text}\\n\\nCorrect Answer: {correct_answer}\"\n",
        "\n",
        "                    data_list.append({\n",
        "                        'id': f'direct_{idx}',\n",
        "                        'title': f\"Medical Question {idx+1}\",\n",
        "                        'question': question,\n",
        "                        'choices': choices_text,\n",
        "                        'correct_answer': correct_answer,\n",
        "                        'full_text': full_text,\n",
        "                        'source': 'MedQA Direct',\n",
        "                        'category': 'Medical Knowledge'\n",
        "                    })\n",
        "\n",
        "                df = pd.DataFrame(data_list)\n",
        "                print(f\"âœ… ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì„±ê³µ! ì´ {len(df)}ê°œ í•­ëª©\")\n",
        "                return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {url} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "            continue\n",
        "\n",
        "    raise Exception(\"ëª¨ë“  ì§ì ‘ ë‹¤ìš´ë¡œë“œ ë°©ë²• ì‹¤íŒ¨\")\n",
        "\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # MedQA ë°ì´í„° ë¡œë”©\n",
        "        medqa_df = load_medqa_dataset(subset_size=500)\n",
        "\n",
        "        if len(medqa_df) == 0:\n",
        "            print(\"âŒ ë¡œë”©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            print(\"ğŸ“‹ ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ íŒŒì‹± ê°€ëŠ¥í•œ í•­ëª©ì´ ì—†ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "        else:\n",
        "            print(\"\\nğŸ“Š ë°ì´í„° ìƒ˜í”Œ:\")\n",
        "            print(medqa_df[['title', 'question', 'correct_answer']].head())\n",
        "            print(f\"\\nğŸ“ˆ ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "            print(f\"- ì´ í•­ëª© ìˆ˜: {len(medqa_df)}\")\n",
        "            print(f\"- ë°ì´í„° ì†ŒìŠ¤: {medqa_df['source'].value_counts().to_dict()}\")\n",
        "            print(f\"- ì¹´í…Œê³ ë¦¬: {medqa_df['category'].value_counts().to_dict()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
        "        print(\"ğŸ“‹ ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\")\n",
        "        print(\"  1. ì¸í„°ë„· ì—°ê²° ìƒíƒœ\")\n",
        "        print(\"  2. Hugging Face datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\")\n",
        "        print(\"  3. ë¡œì»¬ JSONL íŒŒì¼ ì¡´ì¬ ì—¬ë¶€\")\n",
        "        print(\"  4. í•„ìš” ê¶Œí•œ ë° ë°©í™”ë²½ ì„¤ì •\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess"
      },
      "source": [
        "## 4. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50b9f35-332c-484e-f1b0-ca85bfc66b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...\n",
            " í† í° ìˆ˜ í†µê³„:\n",
            "  - í‰ê· : 210.6\n",
            "  - ìµœëŒ€: 523\n",
            "  - ìµœì†Œ: 61\n",
            "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ìµœì¢… ë°ì´í„° ê°œìˆ˜: 500\n"
          ]
        }
      ],
      "source": [
        "def preprocess_medical_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    ì˜ë£Œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "    \"\"\"\n",
        "    # ê°œí–‰ ë¬¸ì ì •ë¦¬\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    # ì•ë’¤ ê³µë°± ì œê±°\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_embedding_with_retry(text: str, model: str = embedding_model, max_retries: int = 3) -> List[float]:\n",
        "    \"\"\"\n",
        "    ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì„ë² ë”© ìƒì„± í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    text = preprocess_medical_text(text)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.embeddings.create(\n",
        "                input=[text],\n",
        "                model=model\n",
        "            )\n",
        "            return response.data[0].embedding\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
        "                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„\n",
        "            else:\n",
        "                print(f\"âŒ ì„ë² ë”© ìƒì„± ìµœì¢… ì‹¤íŒ¨: {e}\")\n",
        "                raise e\n",
        "\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "print(\"ğŸ”„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...\")\n",
        "medqa_df['processed_text'] = medqa_df['full_text'].apply(preprocess_medical_text)\n",
        "\n",
        "# í† í° ìˆ˜ ê³„ì‚°\n",
        "medqa_df['n_tokens'] = medqa_df['processed_text'].apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "print(f\" í† í° ìˆ˜ í†µê³„:\")\n",
        "print(f\"  - í‰ê· : {medqa_df['n_tokens'].mean():.1f}\")\n",
        "print(f\"  - ìµœëŒ€: {medqa_df['n_tokens'].max()}\")\n",
        "print(f\"  - ìµœì†Œ: {medqa_df['n_tokens'].min()}\")\n",
        "\n",
        "# ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ í•„í„°ë§ (í† í° ì œí•œ)\n",
        "original_count = len(medqa_df)\n",
        "medqa_df = medqa_df[medqa_df['n_tokens'] <= max_tokens]\n",
        "filtered_count = len(medqa_df)\n",
        "\n",
        "if original_count != filtered_count:\n",
        "    print(f\"âš ï¸ ê¸´ í…ìŠ¤íŠ¸ {original_count - filtered_count}ê°œ ì œê±°ë¨\")\n",
        "\n",
        "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ìµœì¢… ë°ì´í„° ê°œìˆ˜: {len(medqa_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_embeddings",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfd535b-ccea-4410-829a-68223e7f8f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
            "  ì§„í–‰ë¥ : 0/500 (0.0%)\n",
            "  ì§„í–‰ë¥ : 10/500 (2.0%)\n",
            "  ì§„í–‰ë¥ : 20/500 (4.0%)\n",
            "  ì§„í–‰ë¥ : 30/500 (6.0%)\n",
            "  ì§„í–‰ë¥ : 40/500 (8.0%)\n",
            "  ì§„í–‰ë¥ : 50/500 (10.0%)\n",
            "  ì§„í–‰ë¥ : 60/500 (12.0%)\n",
            "  ì§„í–‰ë¥ : 70/500 (14.0%)\n",
            "  ì§„í–‰ë¥ : 80/500 (16.0%)\n",
            "  ì§„í–‰ë¥ : 90/500 (18.0%)\n",
            "  ì§„í–‰ë¥ : 100/500 (20.0%)\n",
            "  ì§„í–‰ë¥ : 110/500 (22.0%)\n",
            "  ì§„í–‰ë¥ : 120/500 (24.0%)\n",
            "  ì§„í–‰ë¥ : 130/500 (26.0%)\n",
            "  ì§„í–‰ë¥ : 140/500 (28.0%)\n",
            "  ì§„í–‰ë¥ : 150/500 (30.0%)\n",
            "  ì§„í–‰ë¥ : 160/500 (32.0%)\n",
            "  ì§„í–‰ë¥ : 170/500 (34.0%)\n",
            "  ì§„í–‰ë¥ : 180/500 (36.0%)\n",
            "  ì§„í–‰ë¥ : 190/500 (38.0%)\n",
            "  ì§„í–‰ë¥ : 200/500 (40.0%)\n",
            "  ì§„í–‰ë¥ : 210/500 (42.0%)\n",
            "  ì§„í–‰ë¥ : 220/500 (44.0%)\n",
            "  ì§„í–‰ë¥ : 230/500 (46.0%)\n",
            "  ì§„í–‰ë¥ : 240/500 (48.0%)\n",
            "  ì§„í–‰ë¥ : 250/500 (50.0%)\n",
            "  ì§„í–‰ë¥ : 260/500 (52.0%)\n",
            "  ì§„í–‰ë¥ : 270/500 (54.0%)\n",
            "  ì§„í–‰ë¥ : 280/500 (56.0%)\n",
            "  ì§„í–‰ë¥ : 290/500 (58.0%)\n",
            "  ì§„í–‰ë¥ : 300/500 (60.0%)\n",
            "  ì§„í–‰ë¥ : 310/500 (62.0%)\n",
            "  ì§„í–‰ë¥ : 320/500 (64.0%)\n",
            "  ì§„í–‰ë¥ : 330/500 (66.0%)\n",
            "  ì§„í–‰ë¥ : 340/500 (68.0%)\n",
            "  ì§„í–‰ë¥ : 350/500 (70.0%)\n",
            "  ì§„í–‰ë¥ : 360/500 (72.0%)\n",
            "  ì§„í–‰ë¥ : 370/500 (74.0%)\n",
            "  ì§„í–‰ë¥ : 380/500 (76.0%)\n",
            "  ì§„í–‰ë¥ : 390/500 (78.0%)\n",
            "  ì§„í–‰ë¥ : 400/500 (80.0%)\n",
            "  ì§„í–‰ë¥ : 410/500 (82.0%)\n",
            "  ì§„í–‰ë¥ : 420/500 (84.0%)\n",
            "  ì§„í–‰ë¥ : 430/500 (86.0%)\n",
            "  ì§„í–‰ë¥ : 440/500 (88.0%)\n",
            "  ì§„í–‰ë¥ : 450/500 (90.0%)\n",
            "  ì§„í–‰ë¥ : 460/500 (92.0%)\n",
            "  ì§„í–‰ë¥ : 470/500 (94.0%)\n",
            "  ì§„í–‰ë¥ : 480/500 (96.0%)\n",
            "  ì§„í–‰ë¥ : 490/500 (98.0%)\n",
            " ì„ë² ë”© ìƒì„± ì™„ë£Œ! ì´ 500ê°œ í•­ëª©\n",
            " ì„ë² ë”© ì°¨ì›: 1536\n",
            " ë°ì´í„° ì €ì¥ ì™„ë£Œ: medqa_embeddings.csv\n"
          ]
        }
      ],
      "source": [
        "# ì„ë² ë”© ìƒì„±\n",
        "print(\" ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
        "\n",
        "embeddings = []\n",
        "failed_indices = []\n",
        "\n",
        "for idx, text in enumerate(medqa_df['processed_text']):\n",
        "    try:\n",
        "        if idx % 10 == 0:  # ì§„í–‰ìƒí™© í‘œì‹œ\n",
        "            print(f\"  ì§„í–‰ë¥ : {idx}/{len(medqa_df)} ({idx/len(medqa_df)*100:.1f}%)\")\n",
        "\n",
        "        embedding = get_embedding_with_retry(text)\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {idx}): {e}\")\n",
        "        failed_indices.append(idx)\n",
        "        embeddings.append(None)\n",
        "\n",
        "# ì‹¤íŒ¨í•œ í•­ëª© ì œê±°\n",
        "if failed_indices:\n",
        "    print(f\" {len(failed_indices)}ê°œ í•­ëª©ì—ì„œ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\")\n",
        "    valid_mask = [emb is not None for emb in embeddings]\n",
        "    medqa_df = medqa_df[valid_mask].reset_index(drop=True)\n",
        "    embeddings = [emb for emb in embeddings if emb is not None]\n",
        "\n",
        "# ì„ë² ë”©ì„ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
        "medqa_df['embeddings'] = embeddings\n",
        "\n",
        "print(f\" ì„ë² ë”© ìƒì„± ì™„ë£Œ! ì´ {len(medqa_df)}ê°œ í•­ëª©\")\n",
        "print(f\" ì„ë² ë”© ì°¨ì›: {len(embeddings[0]) if embeddings else 'N/A'}\")\n",
        "\n",
        "# ë°ì´í„° ì €ì¥\n",
        "medqa_df.to_csv('medqa_embeddings.csv', index=False, encoding='utf-8')\n",
        "print(\" ë°ì´í„° ì €ì¥ ì™„ë£Œ: medqa_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "search_engine"
      },
      "source": [
        "## 5. ì˜ë£Œ íŠ¹í™” ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "search_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ce6492-159c-4663-96fd-324c599f3038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì˜ë£Œ íŠ¹í™” ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "def to_numpy_embedding(embedding_data) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    ë‹¤ì–‘í•œ í˜•íƒœì˜ ì„ë² ë”© ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
        "    \"\"\"\n",
        "    if isinstance(embedding_data, np.ndarray):\n",
        "        return embedding_data.astype(np.float32)\n",
        "    elif isinstance(embedding_data, list):\n",
        "        return np.array(embedding_data, dtype=np.float32)\n",
        "    elif isinstance(embedding_data, str):\n",
        "        try:\n",
        "            # JSON ë¬¸ìì—´ íŒŒì‹± ì‹œë„\n",
        "            parsed = json.loads(embedding_data)\n",
        "            return np.array(parsed, dtype=np.float32)\n",
        "        except json.JSONDecodeError:\n",
        "            # literal_eval ì‹œë„\n",
        "            parsed = ast.literal_eval(embedding_data)\n",
        "            return np.array(parsed, dtype=np.float32)\n",
        "    else:\n",
        "        return np.array(embedding_data, dtype=np.float32)\n",
        "\n",
        "\n",
        "def calculate_similarity_scores(query_embedding: np.ndarray,\n",
        "                               doc_embeddings: List[np.ndarray],\n",
        "                               metric: str = \"cosine\") -> List[float]:\n",
        "    \"\"\"\n",
        "    ì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    \"\"\"\n",
        "    similarity_functions = {\n",
        "        \"cosine\": lambda q, d: 1 - spatial.distance.cosine(q, d),\n",
        "        \"euclidean\": lambda q, d: 1 / (1 + spatial.distance.euclidean(q, d)),\n",
        "        \"dot_product\": lambda q, d: np.dot(q, d)\n",
        "    }\n",
        "\n",
        "    similarity_func = similarity_functions.get(metric, similarity_functions[\"cosine\"])\n",
        "\n",
        "    scores = []\n",
        "    for doc_emb in doc_embeddings:\n",
        "        try:\n",
        "            score = similarity_func(query_embedding, doc_emb)\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}\")\n",
        "            scores.append(0.0)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def medical_search(query: str,\n",
        "                  df: pd.DataFrame,\n",
        "                  top_k: int = 5,\n",
        "                  category_filter: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ì˜ë£Œ íŠ¹í™” ê²€ìƒ‰ í•¨ìˆ˜\n",
        "\n",
        "    Args:\n",
        "        query: ê²€ìƒ‰ ì§ˆì˜\n",
        "        df: ê²€ìƒ‰í•  ë°ì´í„°í”„ë ˆì„\n",
        "        top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜\n",
        "        category_filter: ì¹´í…Œê³ ë¦¬ í•„í„° (ì˜ˆ: 'Cardiology')\n",
        "\n",
        "    Returns:\n",
        "        ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„\n",
        "    \"\"\"\n",
        "    print(f\" ì˜ë£Œ ê²€ìƒ‰ ìˆ˜í–‰: '{query}'\")\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©\n",
        "    search_df = df.copy()\n",
        "    if category_filter:\n",
        "        search_df = search_df[search_df['category'] == category_filter]\n",
        "        print(f\" ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©: {category_filter} ({len(search_df)}ê°œ í•­ëª©)\")\n",
        "\n",
        "    if len(search_df) == 0:\n",
        "        print(\"âŒ ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
        "        query_embedding = get_embedding_with_retry(query)\n",
        "        query_emb_array = np.array(query_embedding, dtype=np.float32)\n",
        "\n",
        "        # ë¬¸ì„œ ì„ë² ë”© ë³€í™˜\n",
        "        doc_embeddings = [to_numpy_embedding(emb) for emb in search_df['embeddings']]\n",
        "\n",
        "        # ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        similarities = calculate_similarity_scores(query_emb_array, doc_embeddings)\n",
        "\n",
        "        # ê²°ê³¼ ì •ë ¬ ë° ìƒìœ„ kê°œ ë°˜í™˜\n",
        "        search_df = search_df.copy()\n",
        "        search_df['similarity'] = similarities\n",
        "\n",
        "        results = search_df.nlargest(top_k, 'similarity')\n",
        "\n",
        "        print(f\"âœ… ê²€ìƒ‰ ì™„ë£Œ! ìƒìœ„ {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def create_medical_context(search_results: pd.DataFrame, max_length: int = 2000) -> str:\n",
        "    \"\"\"\n",
        "    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ë£Œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
        "    \"\"\"\n",
        "    if len(search_results) == 0:\n",
        "        return \"ê´€ë ¨ ì˜ë£Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    context_parts = []\n",
        "    current_length = 0\n",
        "\n",
        "    for idx, row in search_results.iterrows():\n",
        "        # ê° ê²°ê³¼ì— ëŒ€í•œ ì •ë³´ êµ¬ì„±\n",
        "        part = f\"\"\"[ì˜ë£Œ ì •ë³´ {len(context_parts) + 1}] (ì‹ ë¢°ë„: {row['similarity']:.3f})\n",
        "ì œëª©: {row['title']}\n",
        "ë¶„ì•¼: {row['category']}\n",
        "ë‚´ìš©: {row['processed_text'][:500]}...\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        if current_length + len(part) > max_length:\n",
        "            break\n",
        "\n",
        "        context_parts.append(part)\n",
        "        current_length += len(part)\n",
        "\n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "\n",
        "print(\"âœ… ì˜ë£Œ íŠ¹í™” ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prompt_templates"
      },
      "source": [
        "## 6. ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°œë°œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "medical_prompts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30637e3f-1dfa-46fd-a211-e3441b93ef7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°œë°œ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "class MedicalPromptTemplates:\n",
        "    \"\"\"\n",
        "    ì˜ë£Œ ë¶„ì•¼ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_medical_assistant_prompt(context: str, question: str, query_type: str = \"general\") -> str:\n",
        "        \"\"\"\n",
        "        ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ìš© ê¸°ë³¸ í”„ë¡¬í”„íŠ¸\n",
        "        \"\"\"\n",
        "        base_prompt = f\"\"\"ë‹¹ì‹ ì€ ì˜ë£Œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì˜ë£Œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n",
        "\n",
        "**ì¤‘ìš”í•œ ì§€ì¹¨:**\n",
        "1. ì œê³µëœ ì˜ë£Œ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”\n",
        "2. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” \"ì¶”ê°€ ì˜ë£Œì§„ ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤\"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”\n",
        "3. ì‘ê¸‰ ìƒí™©ì´ ì˜ì‹¬ë˜ë©´ ì¦‰ì‹œ ì‘ê¸‰ì‹¤ ë°©ë¬¸ì„ ê¶Œí•˜ì„¸ìš”\n",
        "4. ê°œì¸ì ì¸ ì˜ë£Œ ì¡°ì–¸ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì˜í•™ ì •ë³´ì„ì„ ëª…ì‹œí•˜ì„¸ìš”\n",
        "5. ì •ë³´ì˜ ì¶œì²˜ì™€ ì‹ ë¢°ë„ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”\n",
        "\n",
        "**ì˜ë£Œ ì§€ì‹ ë² ì´ìŠ¤:**\n",
        "{context}\n",
        "\n",
        "**ì‚¬ìš©ì ì§ˆë¬¸:** {question}\n",
        "\n",
        "**ë‹µë³€ í˜•ì‹:**\n",
        "- ì£¼ìš” ë‹µë³€: [í•µì‹¬ ì˜ë£Œ ì •ë³´]\n",
        "- ê·¼ê±°: [ì°¸ì¡°í•œ ì˜ë£Œ ì§€ì‹ì˜ ì¶œì²˜]\n",
        "- ì£¼ì˜ì‚¬í•­: [ì¶”ê°€ ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì˜ë£Œì§„ ìƒë‹´ í•„ìš”ì„±]\n",
        "- ë©´ì±…ì¡°í•­: ì´ ì •ë³´ëŠ” ì¼ë°˜ì ì¸ ì˜í•™ ì •ë³´ì´ë©°, ê°œì¸ì ì¸ ì˜ë£Œ ìƒë‹´ì„ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë‹µë³€:\"\"\"\n",
        "\n",
        "        return base_prompt\n",
        "\n",
        "    @staticmethod\n",
        "    def get_diagnosis_support_prompt(context: str, symptoms: str) -> str:\n",
        "        \"\"\"\n",
        "        ì§„ë‹¨ ì§€ì›ìš© í”„ë¡¬í”„íŠ¸\n",
        "        \"\"\"\n",
        "        return f\"\"\"ë‹¹ì‹ ì€ ì˜ë£Œ ì§„ë‹¨ ì§€ì› AIì…ë‹ˆë‹¤. ì œì‹œëœ ì¦ìƒì„ ë°”íƒ•ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§„ë‹¨ê³¼ ì¶”ê°€ ê²€ì‚¬ë¥¼ ì œì•ˆí•˜ì„¸ìš”.\n",
        "\n",
        "**ì£¼ì˜:** ì´ëŠ” ì˜ë£Œì§„ì˜ íŒë‹¨ì„ ë•ëŠ” ë„êµ¬ì´ë©°, ìµœì¢… ì§„ë‹¨ì€ ë°˜ë“œì‹œ ì˜ë£Œì§„ì´ ë‚´ë ¤ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì˜ë£Œ ì§€ì‹ ë² ì´ìŠ¤:**\n",
        "{context}\n",
        "\n",
        "**í™˜ì ì¦ìƒ:** {symptoms}\n",
        "\n",
        "**ë¶„ì„ í•­ëª©:**\n",
        "1. ê°€ëŠ¥í•œ ì§„ë‹¨ (í™•ë¥  ìˆœ)\n",
        "2. ê°ë³„ ì§„ë‹¨\n",
        "3. ê¶Œì¥ ì¶”ê°€ ê²€ì‚¬\n",
        "4. ì‘ê¸‰ë„ í‰ê°€\n",
        "5. ì´ˆê¸° ì¹˜ë£Œ ë°©í–¥\n",
        "\n",
        "ë‹µë³€:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_treatment_recommendation_prompt(context: str, diagnosis: str) -> str:\n",
        "        \"\"\"\n",
        "        ì¹˜ë£Œ ê¶Œì¥ì‚¬í•­ìš© í”„ë¡¬í”„íŠ¸\n",
        "        \"\"\"\n",
        "        return f\"\"\"ë‹¹ì‹ ì€ ì˜ë£Œ ì¹˜ë£Œ ê¶Œì¥ì‚¬í•­ ì œê³µ AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§„ë‹¨ì— ëŒ€í•œ í‘œì¤€ ì¹˜ë£Œë²•ì„ ì œì‹œí•˜ì„¸ìš”.\n",
        "\n",
        "**ì˜ë£Œ ì§€ì‹ ë² ì´ìŠ¤:**\n",
        "{context}\n",
        "\n",
        "**ì§„ë‹¨:** {diagnosis}\n",
        "\n",
        "**ì¹˜ë£Œ ê¶Œì¥ì‚¬í•­:**\n",
        "1. 1ì°¨ ì¹˜ë£Œë²• (First-line treatment)\n",
        "2. ëŒ€ì•ˆ ì¹˜ë£Œë²• (Alternative treatments)\n",
        "3. ì•½ë¬¼ ìš©ë²•Â·ìš©ëŸ‰\n",
        "4. ì¹˜ë£Œ ê¸°ê°„ ë° ëª¨ë‹ˆí„°ë§\n",
        "5. ë¶€ì‘ìš© ë° ì£¼ì˜ì‚¬í•­\n",
        "6. ì˜ˆí›„ ë° ì¶”ì ê´€ì°°\n",
        "\n",
        "**ë©´ì±…ì¡°í•­:** ëª¨ë“  ì¹˜ë£ŒëŠ” í™˜ìì˜ ê°œë³„ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì˜ë£Œì§„ì´ ìµœì¢… ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ë‹µë³€:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_drug_information_prompt(context: str, drug_name: str) -> str:\n",
        "        \"\"\"\n",
        "        ì•½ë¬¼ ì •ë³´ ì œê³µìš© í”„ë¡¬í”„íŠ¸\n",
        "        \"\"\"\n",
        "        return f\"\"\"ë‹¹ì‹ ì€ ì•½ë¬¼ ì •ë³´ ì œê³µ AIì…ë‹ˆë‹¤. ìš”ì²­ëœ ì•½ë¬¼ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.\n",
        "\n",
        "**ì˜ë£Œ ì§€ì‹ ë² ì´ìŠ¤:**\n",
        "{context}\n",
        "\n",
        "**ì•½ë¬¼ëª…:** {drug_name}\n",
        "\n",
        "**ì•½ë¬¼ ì •ë³´:**\n",
        "1. ì¼ë°˜ëª… ë° ìƒí’ˆëª…\n",
        "2. ì ì‘ì¦\n",
        "3. ì‘ìš© ê¸°ì „\n",
        "4. ìš©ë²•Â·ìš©ëŸ‰\n",
        "5. ê¸ˆê¸°ì‚¬í•­\n",
        "6. ë¶€ì‘ìš©\n",
        "7. ì•½ë¬¼ ìƒí˜¸ì‘ìš©\n",
        "8. íŠ¹ë³„ ì£¼ì˜ì‚¬í•­\n",
        "\n",
        "**ì•ˆì „ ì •ë³´:** ëª¨ë“  ì•½ë¬¼ ì‚¬ìš©ì€ ì˜ë£Œì§„ì˜ ì²˜ë°©ê³¼ ì§€ì‹œì— ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ë‹µë³€:\"\"\"\n",
        "\n",
        "\n",
        "def generate_medical_response(question: str,\n",
        "                            df: pd.DataFrame,\n",
        "                            response_type: str = \"general\",\n",
        "                            category_filter: str = None) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    ì˜ë£Œ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±\n",
        "\n",
        "    Args:\n",
        "        question: ì‚¬ìš©ì ì§ˆë¬¸\n",
        "        df: ì˜ë£Œ ë°ì´í„° í”„ë ˆì„\n",
        "        response_type: ì‘ë‹µ ìœ í˜• ('general', 'diagnosis', 'treatment', 'drug')\n",
        "        category_filter: ì¹´í…Œê³ ë¦¬ í•„í„°\n",
        "\n",
        "    Returns:\n",
        "        ì‘ë‹µ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ê´€ë ¨ ì˜ë£Œ ì •ë³´ ê²€ìƒ‰\n",
        "        search_results = medical_search(question, df, top_k=3, category_filter=category_filter)\n",
        "\n",
        "        if len(search_results) == 0:\n",
        "            return {\n",
        "                \"answer\": \"ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì˜ë£Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë£Œì§„ì—ê²Œ ì§ì ‘ ìƒë‹´ë°›ìœ¼ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤.\",\n",
        "                \"sources\": [],\n",
        "                \"confidence\": 0.0\n",
        "            }\n",
        "\n",
        "        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
        "        context = create_medical_context(search_results)\n",
        "\n",
        "        # í”„ë¡¬í”„íŠ¸ ì„ íƒ\n",
        "        if response_type == \"diagnosis\":\n",
        "            prompt = MedicalPromptTemplates.get_diagnosis_support_prompt(context, question)\n",
        "        elif response_type == \"treatment\":\n",
        "            prompt = MedicalPromptTemplates.get_treatment_recommendation_prompt(context, question)\n",
        "        elif response_type == \"drug\":\n",
        "            prompt = MedicalPromptTemplates.get_drug_information_prompt(context, question)\n",
        "        else:\n",
        "            prompt = MedicalPromptTemplates.get_medical_assistant_prompt(context, question)\n",
        "\n",
        "        # GPT ì‘ë‹µ ìƒì„±\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.3,  # ì˜ë£Œ ì •ë³´ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ\n",
        "            max_tokens=1500\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content.strip()\n",
        "\n",
        "        # ì¶œì²˜ ì •ë³´ êµ¬ì„±\n",
        "        sources = []\n",
        "        for _, row in search_results.head(3).iterrows():\n",
        "            sources.append({\n",
        "                \"title\": row['title'],\n",
        "                \"category\": row['category'],\n",
        "                \"similarity\": row['similarity'],\n",
        "                \"source\": row['source']\n",
        "            })\n",
        "\n",
        "        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°\n",
        "        avg_confidence = search_results['similarity'].mean() if len(search_results) > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": sources,\n",
        "            \"confidence\": avg_confidence,\n",
        "            \"search_results_count\": len(search_results)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
        "        return {\n",
        "            \"answer\": f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. ì˜ë£Œì§„ì—ê²Œ ì§ì ‘ ìƒë‹´ë°›ìœ¼ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": 0.0\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"âœ… ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°œë°œ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "testing"
      },
      "source": [
        "## 7. ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë° ë°ëª¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "test_system",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2afb85-ef96-4f79-c24c-55ab468c6ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê°œì„ ëœ ì˜ë£Œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
            "ì‹¤ì œ ì˜ë£Œ ë°ì´í„°: 500ê°œ\n",
            "\n",
            "í…ŒìŠ¤íŠ¸ 1/3\n",
            "ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘... (ì´ 500ê°œ)\n",
            "ì„ë² ë”© ì™„ë£Œ\n",
            "ìœ ì‚¬ë„ ë²”ìœ„: -0.150 ~ 0.235\n",
            "ì˜ë£Œ ê´€ë ¨ì„± ë¶„ì„ ì¤‘...\n",
            "  ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒì„¸ ë¶„ì„:\n",
            "      ë§¤ì¹­: ì—”í‹°í‹°(0.000) + í‚¤ì›Œë“œ(0.000) + ë¬¸ë§¥(0.200) = 0.040\n",
            "ì˜ë£Œ ê´€ë ¨ì„± ë²”ìœ„: 0.000 ~ 0.040\n",
            "ì¢…í•© ì ìˆ˜ ë²”ìœ„: -0.090 ~ 0.141\n",
            "ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: 5ê°œ\n",
            "================================================================================\n",
            " ê°œì„ ëœ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ\n",
            "================================================================================\n",
            " ì§ˆë¬¸: ê³ í˜ˆì•• ì¹˜ë£Œì˜ 1ì°¨ ì•½ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
            " ì‹ ë¢°ë„: 0.350 [ë‚®ìŒ]\n",
            " ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: 5\n",
            "\n",
            "--------------------------------------------------\n",
            " ë‹µë³€:\n",
            "--------------------------------------------------\n",
            "ê²€ìƒ‰ëœ ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "--------------------------------------------------\n",
            " ì°¸ì¡° ìë£Œ:\n",
            "--------------------------------------------------\n",
            "1. ì˜ë£Œ ë¬¸ì„œ #59 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.141\n",
            "2. ì˜ë£Œ ë¬¸ì„œ #282 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.116\n",
            "3. ì˜ë£Œ ë¬¸ì„œ #472 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.113\n",
            "\n",
            "ê¶Œì¥ì‚¬í•­: ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜, ì˜ë£Œ ìš©ì–´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.\n",
            "================================================================================\n",
            "\n",
            "ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•œ ì œì•ˆ:\n",
            "  1. ì•½ì˜ ì •í™•í•œ ë³µìš©ë²•ê³¼ ì£¼ì˜ì‚¬í•­ì€?\n",
            "  2. ì•½ì˜ ë¶€ì‘ìš©ê³¼ ëŒ€ì²˜ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
            "\n",
            "í…ŒìŠ¤íŠ¸ 2/3\n",
            "ìœ ì‚¬ë„ ë²”ìœ„: -0.138 ~ 0.240\n",
            "ì˜ë£Œ ê´€ë ¨ì„± ë¶„ì„ ì¤‘...\n",
            "  ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒì„¸ ë¶„ì„:\n",
            "ì˜ë£Œ ê´€ë ¨ì„± ë²”ìœ„: 0.000 ~ 0.013\n",
            "ì¢…í•© ì ìˆ˜ ë²”ìœ„: -0.083 ~ 0.144\n",
            "ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: 5ê°œ\n",
            "================================================================================\n",
            " ê°œì„ ëœ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ\n",
            "================================================================================\n",
            " ì§ˆë¬¸: ë‹¹ë‡¨ë³‘ì˜ ì£¼ìš” ì¦ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
            " ì‹ ë¢°ë„: 0.358 [ë‚®ìŒ]\n",
            " ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: 5\n",
            "\n",
            "--------------------------------------------------\n",
            " ë‹µë³€:\n",
            "--------------------------------------------------\n",
            "ê²€ìƒ‰ëœ ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "--------------------------------------------------\n",
            " ì°¸ì¡° ìë£Œ:\n",
            "--------------------------------------------------\n",
            "1. ì˜ë£Œ ë¬¸ì„œ #59 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.144\n",
            "2. ì˜ë£Œ ë¬¸ì„œ #282 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.124\n",
            "3. ì˜ë£Œ ë¬¸ì„œ #268 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.103\n",
            "\n",
            "ê¶Œì¥ì‚¬í•­: ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜, ì˜ë£Œ ìš©ì–´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.\n",
            "================================================================================\n",
            "\n",
            "ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•œ ì œì•ˆ:\n",
            "  1. ì¦ì˜ ì´ˆê¸° ì¦ìƒê³¼ ì§„ë‹¨ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
            "  2. ì¦ í™˜ìì˜ ìƒí™œ ê´€ë¦¬ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n",
            "\n",
            "í…ŒìŠ¤íŠ¸ 3/3\n",
            "ìœ ì‚¬ë„ ë²”ìœ„: -0.146 ~ 0.186\n",
            "ì˜ë£Œ ê´€ë ¨ì„± ë¶„ì„ ì¤‘...\n",
            "  ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒì„¸ ë¶„ì„:\n",
            "      ë§¤ì¹­: ì—”í‹°í‹°(0.000) + í‚¤ì›Œë“œ(0.000) + ë¬¸ë§¥(0.100) = 0.020\n",
            "ì˜ë£Œ ê´€ë ¨ì„± ë²”ìœ„: 0.000 ~ 0.020\n",
            "ì¢…í•© ì ìˆ˜ ë²”ìœ„: -0.088 ~ 0.112\n",
            "ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: 5ê°œ\n",
            "================================================================================\n",
            " ê°œì„ ëœ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ\n",
            "================================================================================\n",
            " ì§ˆë¬¸: íë ´ ì¹˜ë£Œì— ì‚¬ìš©ë˜ëŠ” í•­ìƒì œëŠ”?\n",
            " ì‹ ë¢°ë„: 0.329 [ë‚®ìŒ]\n",
            " ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: 5\n",
            "\n",
            "--------------------------------------------------\n",
            " ë‹µë³€:\n",
            "--------------------------------------------------\n",
            "ê²€ìƒ‰ëœ ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "--------------------------------------------------\n",
            " ì°¸ì¡° ìë£Œ:\n",
            "--------------------------------------------------\n",
            "1. ì˜ë£Œ ë¬¸ì„œ #212 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.112\n",
            "2. ì˜ë£Œ ë¬¸ì„œ #290 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.103\n",
            "3. ì˜ë£Œ ë¬¸ì„œ #268 [ë‚®ìŒ]\n",
            "   ë¶„ì•¼: Medical Knowledge, ìœ ì‚¬ë„: 0.103\n",
            "\n",
            "ê¶Œì¥ì‚¬í•­: ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜, ì˜ë£Œ ìš©ì–´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.\n",
            "================================================================================\n",
            "\n",
            "ê°œì„ ëœ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
            "ê°œì„ ëœ ì˜ë£Œ RAG ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "ì‚¬ìš©ë²•: run_improved_test(medqa_df)\n",
            "ì£¼ì˜: medqa_dfë¥¼ ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ë³€ìˆ˜ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "class ImprovedMedicalRAG:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        ê°œì„ ëœ ì˜ë£Œ RAG ì‹œìŠ¤í…œ\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "        # ë™ì ìœ¼ë¡œ ì˜ë£Œ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ íŒ¨í„´ë“¤\n",
        "        self.medical_patterns = {\n",
        "            'disease_terms': [\n",
        "                r'(\\w*ë³‘)', r'(\\w*ì—¼)', r'(\\w*ì¦)', r'(\\w*ê²½ìƒ‰)', r'(\\w*ì¢…)',\n",
        "                r'(\\w*ì•”)', r'(\\w*ê¶¤ì–‘)', r'(\\w*ë¶€ì „)', r'(\\w*ì¤‘ë…)',\n",
        "                r'(hypertension)', r'(diabetes)', r'(pneumonia)', r'(infection)',\n",
        "                r'(disease)', r'(syndrome)', r'(disorder)', r'(cancer)'\n",
        "            ],\n",
        "            'medication_terms': [\n",
        "                r'(\\w*ì œ)', r'(\\w*ì•½)', r'(\\w*ì •)', r'(\\w*ìº¡ìŠ)', r'(\\w*ì‹œëŸ½)',\n",
        "                r'(\\w*ì–µì œì œ)', r'(\\w*ì°¨ë‹¨ì œ)', r'(\\w*ê¸¸í•­ì œ)', r'(\\w*ê³„)',\n",
        "                r'(medication)', r'(drug)', r'(medicine)', r'(therapy)',\n",
        "                r'(treatment)', r'(inhibitor)', r'(blocker)'\n",
        "            ],\n",
        "            'symptom_terms': [\n",
        "                r'(\\w*í†µ)', r'(\\w*ì—´)', r'(\\w*ê¸°ì¹¨)', r'(\\w*í˜¸í¡)', r'(\\w*ì–´ì§€)',\n",
        "                r'(\\w*í”¼ë¡œ)', r'(\\w*ë¶€ì¢…)', r'(\\w*ë°œì§„)', r'(\\w*ê°€ë ¤)',\n",
        "                r'(pain)', r'(fever)', r'(cough)', r'(symptom)', r'(fatigue)'\n",
        "            ],\n",
        "            'procedure_terms': [\n",
        "                r'(\\w*ìˆ )', r'(\\w*ê²€ì‚¬)', r'(\\w*ì´¬ì˜)', r'(\\w*ì¸¡ì •)',\n",
        "                r'(\\w*ì¹˜ë£Œ)', r'(\\w*ì²˜ì¹˜)', r'(\\w*ìˆ˜ìˆ )',\n",
        "                r'(surgery)', r'(examination)', r'(procedure)', r'(test)', r'(scan)'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # ë‹¤êµ­ì–´ ì˜ë£Œ ìš©ì–´ë“¤ (í•œêµ­ì–´ + ì˜ì–´)\n",
        "        self.multilingual_medical_terms = {\n",
        "            'diagnosis': ['ì§„ë‹¨', 'ê²€ì§„', 'ì†Œê²¬', 'íŒë…', 'ê²°ê³¼', 'diagnosis', 'examination', 'test', 'result'],\n",
        "            'treatment': ['ì¹˜ë£Œ', 'ì²˜ë°©', 'íˆ¬ì—¬', 'ë³µìš©', 'ì‚¬ìš©', 'ì ìš©', 'treatment', 'therapy', 'prescription', 'medication'],\n",
        "            'prevention': ['ì˜ˆë°©', 'ë°©ì§€', 'ì°¨ë‹¨', 'ë³´í˜¸', 'ê´€ë¦¬', 'prevention', 'protect', 'manage'],\n",
        "            'anatomy': ['ì‹¬ì¥', 'í', 'ê°„', 'ì‹ ì¥', 'ë‡Œ', 'ìœ„', 'ì¥', 'í˜ˆê´€', 'heart', 'lung', 'liver', 'kidney', 'brain'],\n",
        "            'vital_signs': ['í˜ˆì••', 'ë§¥ë°•', 'ì²´ì˜¨', 'í˜¸í¡ìˆ˜', 'ì‚°ì†Œí¬í™”ë„', 'blood pressure', 'pulse', 'temperature', 'bp'],\n",
        "            'lab_values': ['í˜ˆë‹¹', 'ì½œë ˆìŠ¤í…Œë¡¤', 'í—¤ëª¨ê¸€ë¡œë¹ˆ', 'ë°±í˜ˆêµ¬', 'í˜ˆì†ŒíŒ', 'glucose', 'cholesterol', 'hemoglobin'],\n",
        "            'hypertension': ['ê³ í˜ˆì••', 'í˜ˆì••', 'ìˆ˜ì¶•ê¸°', 'ì´ì™„ê¸°', 'hypertension', 'blood pressure', 'systolic', 'diastolic', 'bp'],\n",
        "            'diabetes': ['ë‹¹ë‡¨ë³‘', 'í˜ˆë‹¹', 'ì¸ìŠë¦°', 'diabetes', 'glucose', 'insulin', 'blood sugar'],\n",
        "            'pneumonia': ['íë ´', 'í•­ìƒì œ', 'í', 'ê¸°ì¹¨', 'pneumonia', 'antibiotic', 'lung', 'cough'],\n",
        "            'first_line': ['1ì°¨', 'ì¼ì°¨', 'ì²«ë²ˆì§¸', 'first-line', 'primary', 'initial', 'first choice']\n",
        "        }\n",
        "\n",
        "    def preprocess_question(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° ì˜ë£Œ ìš©ì–´ ì •ê·œí™”\n",
        "        \"\"\"\n",
        "        normalized_question = question.lower()\n",
        "\n",
        "        # ë™ì˜ì–´ ì²˜ë¦¬\n",
        "        synonyms = {\n",
        "            '1ì°¨ ì•½ë¬¼': 'ì¼ì°¨ ì¹˜ë£Œì œ',\n",
        "            'ì£¼ìš” ì¦ìƒ': 'ëŒ€í‘œì ì¸ ì¦ìƒ',\n",
        "            'ë¶€ì‘ìš©': 'ì´ìƒë°˜ì‘',\n",
        "            'ì‘ê¸‰ì²˜ì¹˜': 'ì‘ê¸‰ì¹˜ë£Œ'\n",
        "        }\n",
        "\n",
        "        for original, normalized in synonyms.items():\n",
        "            normalized_question = normalized_question.replace(original, normalized)\n",
        "\n",
        "        return normalized_question\n",
        "\n",
        "    def extract_medical_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        í…ìŠ¤íŠ¸ì—ì„œ ì˜ë£Œ ì—”í‹°í‹°ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œ (ë‹¤êµ­ì–´ ì§€ì›)\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        extracted_entities = {}\n",
        "\n",
        "        for category, patterns in self.medical_patterns.items():\n",
        "            entities = []\n",
        "            for pattern in patterns:\n",
        "                matches = re.findall(pattern, text_lower)\n",
        "                if matches:\n",
        "                    if isinstance(matches[0], tuple):\n",
        "                        entities.extend([match[0] for match in matches if match[0]])\n",
        "                    else:\n",
        "                        entities.extend([match for match in matches if match])\n",
        "            extracted_entities[category] = list(set(entities))\n",
        "\n",
        "        # ë‹¤êµ­ì–´ ì˜ë£Œ ìš©ì–´ ë§¤ì¹­\n",
        "        for category, terms in self.multilingual_medical_terms.items():\n",
        "            matched_terms = [term for term in terms if term in text_lower]\n",
        "            if matched_terms:\n",
        "                extracted_entities[f'multilingual_{category}'] = matched_terms\n",
        "\n",
        "        return extracted_entities\n",
        "\n",
        "    def calculate_medical_relevance_silent(self, question: str, document: str) -> float:\n",
        "        \"\"\"\n",
        "        ì˜ë£Œ ê´€ë ¨ì„± ê³„ì‚° (ë‹¤êµ­ì–´ ì§€ì›, ì¶œë ¥ ì—†ìŒ)\n",
        "        \"\"\"\n",
        "        question_lower = question.lower()\n",
        "        document_lower = document.lower()\n",
        "\n",
        "        # í•µì‹¬ ì˜ë£Œ í‚¤ì›Œë“œë“¤ (í•œì˜ í˜¼í•©)\n",
        "        all_keywords = []\n",
        "        for terms in self.multilingual_medical_terms.values():\n",
        "            all_keywords.extend(terms)\n",
        "\n",
        "        keyword_matches = 0\n",
        "        total_keywords = len(set(all_keywords))\n",
        "\n",
        "        for keyword in set(all_keywords):\n",
        "            if keyword in question_lower and keyword in document_lower:\n",
        "                keyword_matches += 1\n",
        "\n",
        "        keyword_relevance = keyword_matches / total_keywords if total_keywords > 0 else 0\n",
        "\n",
        "        # ì˜ë£Œ ë¬¸ë§¥ ì ìˆ˜ (ë‹¤êµ­ì–´)\n",
        "        medical_context_patterns = [\n",
        "            'patient', 'treatment', 'diagnosis', 'symptom', 'medication', 'therapy',\n",
        "            'hospital', 'doctor', 'nurse', 'surgery', 'examination', 'prescription',\n",
        "            'í™˜ì', 'ì¹˜ë£Œ', 'ì§„ë‹¨', 'ì¦ìƒ', 'ì•½ë¬¼', 'ì²˜ë°©', 'ë³‘ì›', 'ì˜ì‚¬', 'ê°„í˜¸', 'ìˆ˜ìˆ ', 'ê²€ì‚¬'\n",
        "        ]\n",
        "\n",
        "        question_context = sum(1 for pattern in medical_context_patterns\n",
        "                             if pattern in question_lower)\n",
        "        document_context = sum(1 for pattern in medical_context_patterns\n",
        "                             if pattern in document_lower)\n",
        "\n",
        "        if question_context > 0 and document_context > 0:\n",
        "            context_bonus = min(question_context, document_context) / len(medical_context_patterns)\n",
        "        else:\n",
        "            context_bonus = 0\n",
        "\n",
        "        final_score = (keyword_relevance * 0.7) + (context_bonus * 0.3)\n",
        "        return min(final_score, 1.0)\n",
        "\n",
        "    def calculate_medical_relevance(self, question: str, document: str) -> float:\n",
        "        \"\"\"\n",
        "        ë™ì  ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œì„ í†µí•œ ê´€ë ¨ì„± ê³„ì‚° (ê°„ì†Œí™”ëœ ì¶œë ¥)\n",
        "        \"\"\"\n",
        "        # ì—”í‹°í‹° ì¶”ì¶œ\n",
        "        question_entities = self.extract_medical_entities(question)\n",
        "        document_entities = self.extract_medical_entities(document)\n",
        "\n",
        "        # ì—”í‹°í‹° ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°\n",
        "        total_score = 0\n",
        "        category_count = 0\n",
        "        matched_categories = []\n",
        "\n",
        "        for category in question_entities:\n",
        "            if category in document_entities:\n",
        "                q_entities = set(question_entities[category])\n",
        "                d_entities = set(document_entities[category])\n",
        "                common_entities = q_entities.intersection(d_entities)\n",
        "\n",
        "                if q_entities:\n",
        "                    category_score = len(common_entities) / len(q_entities)\n",
        "                    total_score += category_score\n",
        "                    category_count += 1\n",
        "                    if common_entities:\n",
        "                        matched_categories.append(f\"{category}: {list(common_entities)[:2]}\")\n",
        "\n",
        "        entity_relevance = total_score / category_count if category_count > 0 else 0\n",
        "\n",
        "        # ë‹¤êµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­\n",
        "        question_lower = question.lower()\n",
        "        document_lower = document.lower()\n",
        "\n",
        "        keyword_matches = 0\n",
        "        matched_keywords = []\n",
        "\n",
        "        # í•µì‹¬ í‚¤ì›Œë“œë“¤ë§Œ í™•ì¸\n",
        "        core_keywords = ['ê³ í˜ˆì••', 'hypertension', 'ë‹¹ë‡¨ë³‘', 'diabetes', 'íë ´', 'pneumonia',\n",
        "                        'ì¹˜ë£Œ', 'treatment', 'ì•½ë¬¼', 'medication', 'ì¦ìƒ', 'symptom',\n",
        "                        'blood pressure', 'í˜ˆì••', '1ì°¨', 'first-line', 'primary']\n",
        "\n",
        "        for keyword in core_keywords:\n",
        "            if keyword in question_lower and keyword in document_lower:\n",
        "                keyword_matches += 1\n",
        "                matched_keywords.append(keyword)\n",
        "\n",
        "        keyword_relevance = keyword_matches / len(core_keywords)\n",
        "\n",
        "        # ì˜ë£Œ ë¬¸ë§¥ ì ìˆ˜\n",
        "        context_patterns = [\n",
        "            'patient', 'treatment', 'therapy', 'medication', 'diagnosis',\n",
        "            'í™˜ì', 'ì¹˜ë£Œ', 'ì•½ë¬¼', 'ì§„ë‹¨', 'ì²˜ë°©'\n",
        "        ]\n",
        "\n",
        "        question_context = sum(1 for pattern in context_patterns if pattern in question_lower)\n",
        "        document_context = sum(1 for pattern in context_patterns if pattern in document_lower)\n",
        "\n",
        "        context_bonus = 0\n",
        "        if question_context > 0 and document_context > 0:\n",
        "            context_bonus = min(question_context, document_context) / len(context_patterns)\n",
        "\n",
        "        # ìµœì¢… ì ìˆ˜\n",
        "        final_score = (entity_relevance * 0.4) + (keyword_relevance * 0.4) + (context_bonus * 0.2)\n",
        "\n",
        "        # ê°„ë‹¨í•œ ë””ë²„ê¹… ì •ë³´\n",
        "        if final_score > 0:\n",
        "            print(f\"      ë§¤ì¹­: ì—”í‹°í‹°({entity_relevance:.3f}) + í‚¤ì›Œë“œ({keyword_relevance:.3f}) + ë¬¸ë§¥({context_bonus:.3f}) = {final_score:.3f}\")\n",
        "            if matched_keywords:\n",
        "                print(f\"      í‚¤ì›Œë“œ ë§¤ì¹­: {matched_keywords[:3]}\")\n",
        "\n",
        "        return min(final_score, 1.0)\n",
        "\n",
        "    def enhanced_search(self, question: str, df: pd.DataFrame, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥ (ë‹¤êµ­ì–´ ì§€ì›)\n",
        "        \"\"\"\n",
        "        # ì§ˆë¬¸ ì „ì²˜ë¦¬\n",
        "        processed_question = self.preprocess_question(question)\n",
        "\n",
        "        # ì„ë² ë”© ê³„ì‚°\n",
        "        question_embedding = self.model.encode([processed_question])\n",
        "\n",
        "        # ë¬¸ì„œ ì„ë² ë”© (ìºì‹± ê°€ëŠ¥)\n",
        "        if not hasattr(self, '_document_embeddings'):\n",
        "            if 'context' in df.columns:\n",
        "                documents = df['context'].fillna('').tolist()\n",
        "            else:\n",
        "                documents = []\n",
        "                for _, row in df.iterrows():\n",
        "                    question_text = row.get('question', '')\n",
        "                    answer_text = row.get('answer', '')\n",
        "                    combined_text = f\"{question_text} {answer_text}\"\n",
        "                    documents.append(combined_text)\n",
        "\n",
        "            print(f\"ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(documents)}ê°œ)\")\n",
        "            self._document_embeddings = self.model.encode(documents)\n",
        "            print(\"ì„ë² ë”© ì™„ë£Œ\")\n",
        "\n",
        "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        similarities = cosine_similarity(question_embedding, self._document_embeddings)[0]\n",
        "        print(f\"ìœ ì‚¬ë„ ë²”ìœ„: {similarities.min():.3f} ~ {similarities.max():.3f}\")\n",
        "\n",
        "        # ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        medical_relevances = []\n",
        "        print(\"ì˜ë£Œ ê´€ë ¨ì„± ë¶„ì„ ì¤‘...\")\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            if 'context' in df.columns:\n",
        "                document_text = row.get('context', '')\n",
        "            else:\n",
        "                document_text = f\"{row.get('question', '')} {row.get('answer', '')}\"\n",
        "\n",
        "            # ì²« ë²ˆì§¸ ë¬¸ì„œë§Œ ìƒì„¸ ë¶„ì„\n",
        "            if idx == 0:\n",
        "                print(\"  ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒì„¸ ë¶„ì„:\")\n",
        "                med_relevance = self.calculate_medical_relevance(question, document_text)\n",
        "            else:\n",
        "                med_relevance = self.calculate_medical_relevance_silent(question, document_text)\n",
        "\n",
        "            medical_relevances.append(med_relevance)\n",
        "\n",
        "            # ì²˜ìŒ ëª‡ ê°œë§Œ ì²˜ë¦¬\n",
        "            if idx >= 9:  # 10ê°œë§Œ ì²˜ë¦¬\n",
        "                break\n",
        "\n",
        "        # ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì±„ì›€\n",
        "        while len(medical_relevances) < len(df):\n",
        "            medical_relevances.append(0.0)\n",
        "\n",
        "        medical_relevances = np.array(medical_relevances)\n",
        "        print(f\"ì˜ë£Œ ê´€ë ¨ì„± ë²”ìœ„: {medical_relevances.min():.3f} ~ {medical_relevances.max():.3f}\")\n",
        "\n",
        "        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ 60% + ì˜ë£Œ ê´€ë ¨ì„± 40%)\n",
        "        combined_scores = similarities * 0.6 + medical_relevances * 0.4\n",
        "        print(f\"ì¢…í•© ì ìˆ˜ ë²”ìœ„: {combined_scores.min():.3f} ~ {combined_scores.max():.3f}\")\n",
        "\n",
        "        # ìƒìœ„ ê²°ê³¼ ì„ íƒ\n",
        "        top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if combined_scores[idx] > 0.05:\n",
        "                if 'context' in df.columns:\n",
        "                    content = df.iloc[idx].get('context', '')\n",
        "                else:\n",
        "                    content = f\"{df.iloc[idx].get('question', '')} {df.iloc[idx].get('answer', '')}\"\n",
        "\n",
        "                results.append({\n",
        "                    'index': int(idx),\n",
        "                    'similarity': float(combined_scores[idx]),\n",
        "                    'content': content,\n",
        "                    'question_original': df.iloc[idx].get('question', ''),\n",
        "                    'answer': df.iloc[idx].get('answer', ''),\n",
        "                    'category': df.iloc[idx].get('category', 'general')\n",
        "                })\n",
        "\n",
        "        print(f\"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ\")\n",
        "        return results\n",
        "\n",
        "    def calculate_confidence(self, search_results: List[Dict], question: str) -> float:\n",
        "        \"\"\"\n",
        "        ì‹ ë¢°ë„ ê³„ì‚° ê°œì„  (ì˜ë£Œ íŠ¹í™”)\n",
        "        \"\"\"\n",
        "        if not search_results:\n",
        "            return 0.0\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        # 1. í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ì¦ê°€)\n",
        "        avg_similarity = np.mean([result['similarity'] for result in search_results])\n",
        "        scores.append(avg_similarity)\n",
        "\n",
        "        # 2. ìµœê³  ì ìˆ˜ ê¸°ì¤€ (ìƒìœ„ ê²°ê³¼ í’ˆì§ˆ)\n",
        "        max_similarity = max(result['similarity'] for result in search_results)\n",
        "        scores.append(max_similarity)\n",
        "\n",
        "        # 3. ê²°ê³¼ ìˆ˜ ê¸°ë°˜ ì ìˆ˜\n",
        "        result_count_score = min(len(search_results) / 3.0, 1.0)  # 3ê°œ ì´ìƒì´ë©´ ìµœê³ ì \n",
        "        scores.append(result_count_score)\n",
        "\n",
        "        # 4. ì˜ë£Œ ì „ë¬¸ì„± ì ìˆ˜ (ë™ì  ê³„ì‚°)\n",
        "        medical_score = 0\n",
        "        for result in search_results:\n",
        "            content = result['content']\n",
        "            entities = self.extract_medical_entities(content)\n",
        "            entity_count = sum(len(entity_list) for entity_list in entities.values())\n",
        "            medical_score += min(entity_count / 5.0, 1.0)  # 5ê°œ ì´ìƒì´ë©´ ë§Œì \n",
        "\n",
        "        medical_score = medical_score / len(search_results) if search_results else 0\n",
        "        scores.append(medical_score)\n",
        "\n",
        "        # 5. ì˜ë£Œ ê´€ë ¨ì„± ë³´ë„ˆìŠ¤ (ìƒˆë¡œ ì¶”ê°€)\n",
        "        question_lower = question.lower()\n",
        "        has_medical_intent = any(keyword in question_lower for keyword in\n",
        "                               ['ê³ í˜ˆì••', 'ë‹¹ë‡¨ë³‘', 'íë ´', 'ì¹˜ë£Œ', 'ì•½ë¬¼', 'ì¦ìƒ', 'ì§„ë‹¨'])\n",
        "\n",
        "        if has_medical_intent and max_similarity > 0.15:\n",
        "            medical_bonus = min(max_similarity * 2, 0.3)  # ìµœëŒ€ 0.3 ë³´ë„ˆìŠ¤\n",
        "        else:\n",
        "            medical_bonus = 0\n",
        "\n",
        "        scores.append(medical_bonus)\n",
        "\n",
        "        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚° (ì˜ë£Œ ê´€ë ¨ì„±ì„ ë” ì¤‘ì‹œ)\n",
        "        weights = [0.25, 0.3, 0.15, 0.2, 0.1]  # ìµœê³ ì ìˆ˜ì™€ ì˜ë£Œì „ë¬¸ì„±ì„ ì¤‘ì‹œ\n",
        "        final_confidence = sum(score * weight for score, weight in zip(scores, weights))\n",
        "\n",
        "        return min(final_confidence, 1.0)\n",
        "\n",
        "    def generate_response(self, question: str, df: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        ê°œì„ ëœ ì‘ë‹µ ìƒì„±\n",
        "        \"\"\"\n",
        "        # ê²€ìƒ‰ ìˆ˜í–‰\n",
        "        search_results = self.enhanced_search(question, df, top_k=5)\n",
        "\n",
        "        # ì‹ ë¢°ë„ ê³„ì‚°\n",
        "        confidence = self.calculate_confidence(search_results, question)\n",
        "\n",
        "        # ë‹µë³€ ìƒì„± ë¡œì§\n",
        "        if confidence < 0.5:\n",
        "            answer = \"ê²€ìƒ‰ëœ ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\"\n",
        "            recommendation = \"ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜, ì˜ë£Œ ìš©ì–´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.\"\n",
        "        else:\n",
        "            # ìƒìœ„ ê²°ê³¼ë“¤ë¡œë¶€í„° ë‹µë³€ êµ¬ì„±\n",
        "            best_results = search_results[:3]\n",
        "            answer_parts = []\n",
        "\n",
        "            for result in best_results:\n",
        "                if result['similarity'] > 0.15:  # ì„ê³„ê°’ ë‚®ì¶¤\n",
        "                    answer_parts.append(result['answer'])\n",
        "\n",
        "            if answer_parts:\n",
        "                answer = \" \".join(set(answer_parts))  # ì¤‘ë³µ ì œê±°\n",
        "            else:\n",
        "                answer = \"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ì§€ë§Œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "            recommendation = \"ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.\"\n",
        "\n",
        "        return {\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'confidence': confidence,\n",
        "            'search_results_count': len(search_results),\n",
        "            'sources': [\n",
        "                {\n",
        "                    'title': f\"ì˜ë£Œ ë¬¸ì„œ #{result['index']}\",\n",
        "                    'similarity': result['similarity'],\n",
        "                    'category': result['category']\n",
        "                }\n",
        "                for result in search_results[:3]\n",
        "            ],\n",
        "            'recommendation': recommendation\n",
        "        }\n",
        "\n",
        "def display_improved_response(response_data: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    ê°œì„ ëœ ì‘ë‹µ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥ (ì´ëª¨í‹°ì½˜ ì œê±°)\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\" ê°œì„ ëœ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\" ì§ˆë¬¸: {response_data['question']}\")\n",
        "\n",
        "    confidence = response_data['confidence']\n",
        "    if confidence > 0.7:\n",
        "        status = \"[ë†’ìŒ]\"\n",
        "    elif confidence > 0.5:\n",
        "        status = \"[ë³´í†µ]\"\n",
        "    else:\n",
        "        status = \"[ë‚®ìŒ]\"\n",
        "\n",
        "    print(f\" ì‹ ë¢°ë„: {confidence:.3f} {status}\")\n",
        "    print(f\" ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {response_data['search_results_count']}\")\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\" ë‹µë³€:\")\n",
        "    print(\"-\"*50)\n",
        "    print(response_data['answer'])\n",
        "\n",
        "    if response_data['sources']:\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\" ì°¸ì¡° ìë£Œ:\")\n",
        "        print(\"-\"*50)\n",
        "        for i, source in enumerate(response_data['sources'], 1):\n",
        "            if source['similarity'] > 0.7:\n",
        "                confidence_level = \"[ë†’ìŒ]\"\n",
        "            elif source['similarity'] > 0.5:\n",
        "                confidence_level = \"[ë³´í†µ]\"\n",
        "            else:\n",
        "                confidence_level = \"[ë‚®ìŒ]\"\n",
        "            print(f\"{i}. {source['title']} {confidence_level}\")\n",
        "            print(f\"   ë¶„ì•¼: {source['category']}, ìœ ì‚¬ë„: {source['similarity']:.3f}\")\n",
        "\n",
        "    print(f\"\\nê¶Œì¥ì‚¬í•­: {response_data['recommendation']}\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "\n",
        "def suggest_better_questions(original_question: str, rag_system) -> List[str]:\n",
        "    \"\"\"\n",
        "    ë™ì  ì˜ë£Œ ì—”í‹°í‹° ë¶„ì„ì„ í†µí•œ ì§ˆë¬¸ ê°œì„  ì œì•ˆ\n",
        "    \"\"\"\n",
        "    suggestions = []\n",
        "\n",
        "    # ì›ë³¸ ì§ˆë¬¸ì—ì„œ ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œ\n",
        "    entities = rag_system.extract_medical_entities(original_question)\n",
        "\n",
        "    # ì§ˆë¬¸ì˜ êµ¬ì²´ì„± ë¶„ì„\n",
        "    question_lower = original_question.lower()\n",
        "\n",
        "    # ë„ˆë¬´ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¸ì§€ ì²´í¬\n",
        "    general_patterns = ['ë¬´ì—‡', 'ì–´ë–¤', 'ì–¸ì œ', 'ì–´ë–»ê²Œ', 'ì™œ']\n",
        "    is_general = any(pattern in question_lower for pattern in general_patterns)\n",
        "\n",
        "    # ì˜ë£Œ ì—”í‹°í‹°ê°€ ìˆëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì œì•ˆ\n",
        "    if entities and is_general:\n",
        "        # ì§ˆë³‘ ê´€ë ¨ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´\n",
        "        if 'disease_terms' in entities and entities['disease_terms']:\n",
        "            disease = entities['disease_terms'][0]\n",
        "            suggestions.extend([\n",
        "                f\"{disease}ì˜ ì´ˆê¸° ì¦ìƒê³¼ ì§„ë‹¨ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "                f\"{disease} í™˜ìì˜ ìƒí™œ ê´€ë¦¬ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\",\n",
        "                f\"{disease}ì˜ í•©ë³‘ì¦ ì˜ˆë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
        "            ])\n",
        "\n",
        "        # ì•½ë¬¼ ê´€ë ¨ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´\n",
        "        if 'medication_terms' in entities and entities['medication_terms']:\n",
        "            medication = entities['medication_terms'][0]\n",
        "            suggestions.extend([\n",
        "                f\"{medication}ì˜ ì •í™•í•œ ë³µìš©ë²•ê³¼ ì£¼ì˜ì‚¬í•­ì€?\",\n",
        "                f\"{medication}ì˜ ë¶€ì‘ìš©ê³¼ ëŒ€ì²˜ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "                f\"{medication}ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë‹¤ë¥¸ ì•½ë¬¼ì€?\"\n",
        "            ])\n",
        "\n",
        "        # ì¦ìƒ ê´€ë ¨ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´\n",
        "        if 'symptom_terms' in entities and entities['symptom_terms']:\n",
        "            symptom = entities['symptom_terms'][0]\n",
        "            suggestions.extend([\n",
        "                f\"{symptom} ì¦ìƒì´ ë‚˜íƒ€ë‚˜ëŠ” ì£¼ìš” ì§ˆí™˜ì€?\",\n",
        "                f\"{symptom}ì´ ì§€ì†ë  ë•Œ ì‘ê¸‰ ìƒí™© íŒë‹¨ ê¸°ì¤€ì€?\",\n",
        "                f\"{symptom} ì™„í™”ë¥¼ ìœ„í•œ ì¦‰ì‹œ ì¡°ì¹˜ë²•ì€?\"\n",
        "            ])\n",
        "\n",
        "    # ì¼ë°˜ì ì¸ ê°œì„  ì œì•ˆ (ì—”í‹°í‹°ê°€ ì—†ê±°ë‚˜ ë§¤ìš° ëª¨í˜¸í•œ ê²½ìš°)\n",
        "    elif is_general:\n",
        "        suggestions.extend([\n",
        "            \"êµ¬ì²´ì ì¸ ì§ˆë³‘ëª…, ì•½ë¬¼ëª…, ë˜ëŠ” ì¦ìƒì„ í¬í•¨í•´ ì£¼ì„¸ìš”\",\n",
        "            \"í™˜ìì˜ ë‚˜ì´, ì„±ë³„, ê¸°ì¡´ ë³‘ë ¥ ë“± ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”\",\n",
        "            \"ê¶ê¸ˆí•œ ì ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•´ ì£¼ì„¸ìš” (ì˜ˆ: ìš©ëŸ‰, ê¸°ê°„, ë°©ë²• ë“±)\"\n",
        "        ])\n",
        "\n",
        "    # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœëŒ€ 3ê°œë§Œ ë°˜í™˜\n",
        "    unique_suggestions = list(dict.fromkeys(suggestions))\n",
        "    return unique_suggestions[:3]\n",
        "\n",
        "def run_improved_test(medical_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    ê°œì„ ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹¤ì œ ë°ì´í„° ì‚¬ìš©)\n",
        "    \"\"\"\n",
        "    # ê°œì„ ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "    rag_system = ImprovedMedicalRAG()\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤\n",
        "    test_questions = [\n",
        "        \"ê³ í˜ˆì•• ì¹˜ë£Œì˜ 1ì°¨ ì•½ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "        \"ë‹¹ë‡¨ë³‘ì˜ ì£¼ìš” ì¦ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "        \"íë ´ ì¹˜ë£Œì— ì‚¬ìš©ë˜ëŠ” í•­ìƒì œëŠ”?\"\n",
        "    ]\n",
        "\n",
        "    print(\"ê°œì„ ëœ ì˜ë£Œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
        "    print(f\"ì‹¤ì œ ì˜ë£Œ ë°ì´í„°: {len(medical_df)}ê°œ\")\n",
        "    print()\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"í…ŒìŠ¤íŠ¸ {i}/{len(test_questions)}\")\n",
        "\n",
        "        # ì‘ë‹µ ìƒì„±\n",
        "        response = rag_system.generate_response(question, medical_df)\n",
        "        display_improved_response(response)\n",
        "\n",
        "        # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ê°œì„ ëœ ì§ˆë¬¸ ì œì•ˆ\n",
        "        if response['confidence'] < 0.6:\n",
        "            suggestions = suggest_better_questions(question, rag_system)\n",
        "            if suggestions:\n",
        "                print(\"ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•œ ì œì•ˆ:\")\n",
        "                for j, suggestion in enumerate(suggestions[:2], 1):\n",
        "                    print(f\"  {j}. {suggestion}\")\n",
        "                print()\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(\"ê°œì„ ëœ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "run_improved_test(medqa_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ê°œì„ ëœ ì˜ë£Œ RAG ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ì‚¬ìš©ë²•: run_improved_test(medqa_df)\")\n",
        "    print(\"ì£¼ì˜: medqa_dfë¥¼ ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ë³€ìˆ˜ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive"
      },
      "source": [
        "## 8. ì¸í„°ë™í‹°ë¸Œ ì˜ë£Œ ì±—ë´‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "interactive_chatbot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0048eb2a-191b-486f-9c8b-2bfeb5b59730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì˜ë£Œ ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
            " ì£¼ì˜: ì½”ë© í™˜ê²½ì—ì„œëŠ” input() í•¨ìˆ˜ê°€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            " ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì‹œë©´ ì™„ì „í•œ ëŒ€í™”í˜• ê²½í—˜ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            " ìƒ˜í”Œ ì§ˆë¬¸ìœ¼ë¡œ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤:\n",
            "\n",
            "ğŸ‘¤ ì§ˆë¬¸: ê³ í˜ˆì••ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\n",
            " ì˜ë£Œ ê²€ìƒ‰ ìˆ˜í–‰: 'ê³ í˜ˆì••ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?'\n",
            "âœ… ê²€ìƒ‰ ì™„ë£Œ! ìƒìœ„ 3ê°œ ê²°ê³¼ ë°˜í™˜\n",
            " ë‹µë³€: - ì£¼ìš” ë‹µë³€: ê³ í˜ˆì••ì€ í˜ˆì••ì´ ì •ìƒ ë²”ìœ„ë¥¼ ì´ˆê³¼í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ë†’ì€ ìƒíƒœë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ì¶•ê¸° í˜ˆì••ì´ 140mmHg ì´ìƒì´ê±°ë‚˜ ì´ì™„ê¸° í˜ˆì••ì´ 90mmHg ì´ìƒì¼ ë•Œ ê³ í˜ˆì••ìœ¼ë¡œ ì§„ë‹¨ë©ë‹ˆë‹¤. ê³ í˜ˆì••ì€ ì‹¬ì¥ë³‘, ë‡Œì¡¸ì¤‘, ì‹ ì¥ ì§ˆí™˜ ë“± ì—¬ëŸ¬ ê±´ê°• ë¬¸ì œì˜ ìœ„í—˜ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "- ê·¼ê±°: [ì˜ë£Œ ì •ë³´ 3] (ì‹ ë¢°ë„: 0.173)ì—ì„œ ê³ í˜ˆì•• í™˜ìì˜...\n",
            " ì‹ ë¢°ë„: 0.183\n",
            "\n",
            "ğŸ‘¤ ì§ˆë¬¸: ìš°ìš¸ì¦ í™˜ìì˜ ìˆ˜ë©´ì¥ì• ëŠ” ì–´ë–»ê²Œ ì¹˜ë£Œí•˜ë‚˜ìš”?\n",
            " ì˜ë£Œ ê²€ìƒ‰ ìˆ˜í–‰: 'ìš°ìš¸ì¦ í™˜ìì˜ ìˆ˜ë©´ì¥ì• ëŠ” ì–´ë–»ê²Œ ì¹˜ë£Œí•˜ë‚˜ìš”?'\n",
            "âœ… ê²€ìƒ‰ ì™„ë£Œ! ìƒìœ„ 3ê°œ ê²°ê³¼ ë°˜í™˜\n",
            " ë‹µë³€: - ì£¼ìš” ë‹µë³€: ìš°ìš¸ì¦ í™˜ìì˜ ìˆ˜ë©´ì¥ì• ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¸ì§€í–‰ë™ì¹˜ë£Œ(CBT)ì™€ ì•½ë¬¼ì¹˜ë£Œ(í•­ìš°ìš¸ì œ ë˜ëŠ” ìˆ˜ë©´ì œ)ë¥¼ í†µí•´ ì¹˜ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CBTëŠ” ìˆ˜ë©´ ìœ„ìƒì„ ê°œì„ í•˜ê³  ë¶€ì •ì ì¸ ìƒê°ì„ ë³€í™”ì‹œí‚¤ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•½ë¬¼ì¹˜ë£ŒëŠ” í™˜ìì˜ ì¦ìƒì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "- ê·¼ê±°: ì œê³µëœ ì˜ë£Œ ì§€ì‹ ë² ì´ìŠ¤ì—ëŠ” ìš°ìš¸ì¦ê³¼ ê´€ë ¨ëœ ìˆ˜ë©´ì¥ì• ì— ëŒ€í•œ êµ¬ì²´ì ...\n",
            " ì‹ ë¢°ë„: 0.288\n",
            "\n",
            "ğŸ‘¤ ì§ˆë¬¸: ì†Œì•„ì˜ ì£¼ê¸°ì„± êµ¬í† ì¦ ì§„ë‹¨ì„ ì§„ë‹¨í•´ì£¼ì„¸ìš”.\n",
            " ì˜ë£Œ ê²€ìƒ‰ ìˆ˜í–‰: 'ì†Œì•„ì˜ ì£¼ê¸°ì„± êµ¬í† ì¦ ì§„ë‹¨ì„ ì§„ë‹¨í•´ì£¼ì„¸ìš”.'\n",
            "âœ… ê²€ìƒ‰ ì™„ë£Œ! ìƒìœ„ 3ê°œ ê²°ê³¼ ë°˜í™˜\n",
            " ë‹µë³€: - ì£¼ìš” ë‹µë³€: ì†Œì•„ì˜ ì£¼ê¸°ì„± êµ¬í† ì¦(Periodic Vomiting Syndrome, PVS)ì€ ë°˜ë³µì ì¸ êµ¬í†  ì—í”¼ì†Œë“œê°€ ë°œìƒí•˜ëŠ” ìƒíƒœë¡œ, ì£¼ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤, ê°ì •ì  ìš”ì¸ ë˜ëŠ” íŠ¹ì • ìŒì‹ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§„ë‹¨ì€ ì„ìƒì  í‰ê°€ì™€ ì¦ìƒ ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë©°, ë‹¤ë¥¸ ì›ì¸ì„ ë°°ì œí•˜ê¸° ìœ„í•œ ì¶”ê°€ ê²€ì‚¬ë„ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "- ê·¼ê±°: ì œê³µëœ ì˜ë£Œ ì •ë³´...\n",
            " ì‹ ë¢°ë„: 0.141\n"
          ]
        }
      ],
      "source": [
        "def run_medical_chatbot(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    ì¸í„°ë™í‹°ë¸Œ ì˜ë£Œ ì±—ë´‡ ì‹¤í–‰\n",
        "    \"\"\"\n",
        "    print(\" ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\")\n",
        "    print(\" ì˜ë£Œ ê´€ë ¨ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ í•´ë³´ì„¸ìš”.\")\n",
        "    print(\"âš ï¸  ì´ AIëŠ” ì¼ë°˜ì ì¸ ì˜í•™ ì •ë³´ë§Œ ì œê³µí•˜ë©°, ì‹¤ì œ ì˜ë£Œ ìƒë‹´ì„ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "    print(\" ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    conversation_history = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # ì‚¬ìš©ì ì…ë ¥\n",
        "            user_input = input(\"\\nğŸ‘¤ ì§ˆë¬¸: \").strip()\n",
        "\n",
        "            # ì¢…ë£Œ ì¡°ê±´\n",
        "            if user_input.lower() in ['exit', 'quit', 'ì¢…ë£Œ', 'q']:\n",
        "                print(\"\\n ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì´ìš©í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                print(\"â“ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\")\n",
        "                continue\n",
        "\n",
        "            print(\"\\nğŸ” ê²€ìƒ‰ ì¤‘...\")\n",
        "\n",
        "            # ì‘ë‹µ ìƒì„±\n",
        "            response = generate_medical_response(\n",
        "                question=user_input,\n",
        "                df=df,\n",
        "                response_type=\"general\"\n",
        "            )\n",
        "\n",
        "            # ì‘ë‹µ ì¶œë ¥\n",
        "            print(f\"\\nğŸ¤– ì˜ë£Œ AI (ì‹ ë¢°ë„: {response['confidence']:.3f}):\")\n",
        "            print(\"-\" * 50)\n",
        "            print(response['answer'])\n",
        "\n",
        "            # ì°¸ì¡° ìë£Œ ì¶œë ¥ (ê°„ë‹¨íˆ)\n",
        "            if response['sources']:\n",
        "                print(f\"\\n ì°¸ì¡° ìë£Œ: {len(response['sources'])}ê°œ\")\n",
        "                for i, source in enumerate(response['sources'][:2], 1):  # ìƒìœ„ 2ê°œë§Œ\n",
        "                    print(f\"   {i}. {source['title']} ({source['category']})\")\n",
        "\n",
        "            # ëŒ€í™” ê¸°ë¡ ì €ì¥\n",
        "            conversation_history.append({\n",
        "                \"user\": user_input,\n",
        "                \"assistant\": response['answer'],\n",
        "                \"confidence\": response['confidence']\n",
        "            })\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n ëŒ€í™”ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
        "            print(\"ğŸ”„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.\")\n",
        "\n",
        "\n",
        "# ì±—ë´‡ ì‹¤í–‰\n",
        "print(\" ì˜ë£Œ ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "print(\" ì£¼ì˜: ì½”ë© í™˜ê²½ì—ì„œëŠ” input() í•¨ìˆ˜ê°€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "print(\" ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì‹œë©´ ì™„ì „í•œ ëŒ€í™”í˜• ê²½í—˜ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ì½”ë© í™˜ê²½ì„ ê³ ë ¤í•œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
        "sample_queries = [\n",
        "    \"ê³ í˜ˆì••ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "    \"ìš°ìš¸ì¦ í™˜ìì˜ ìˆ˜ë©´ì¥ì• ëŠ” ì–´ë–»ê²Œ ì¹˜ë£Œí•˜ë‚˜ìš”?\",\n",
        "    \"ì†Œì•„ì˜ ì£¼ê¸°ì„± êµ¬í† ì¦ ì§„ë‹¨ì„ ì§„ë‹¨í•´ì£¼ì„¸ìš”.\"\n",
        "]\n",
        "\n",
        "print(\"\\n ìƒ˜í”Œ ì§ˆë¬¸ìœ¼ë¡œ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤:\")\n",
        "for query in sample_queries:\n",
        "    print(f\"\\nğŸ‘¤ ì§ˆë¬¸: {query}\")\n",
        "    response = generate_medical_response(query, medqa_df)\n",
        "    print(f\" ë‹µë³€: {response['answer'][:200]}...\")\n",
        "    print(f\" ì‹ ë¢°ë„: {response['confidence']:.3f}\")\n",
        "\n",
        "# ì‹¤ì œ ëŒ€í™”í˜• ëª¨ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ (ì½”ë© í™˜ê²½ ê³ ë ¤)\n",
        "# run_medical_chatbot(medqa_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## 9. ê²°ê³¼ ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "summary_stats",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385f9379-cb88-4b65-cec2-00173cae376b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MedQA ê¸°ë°˜ ì˜ë£Œ RAG ì‹œìŠ¤í…œ - Phase 1 ì™„ë£Œ ë³´ê³ ì„œ\n",
            "================================================================================\n",
            " ë°ì´í„° í†µê³„:\n",
            "  â€¢ ì´ ì˜ë£Œ ë¬¸ì„œ ìˆ˜: 500ê°œ\n",
            "  â€¢ í‰ê·  í† í° ìˆ˜: 210.6\n",
            "  â€¢ ìµœëŒ€ í† í° ìˆ˜: 523\n",
            "  â€¢ ë°ì´í„° ì¶œì²˜: {'GBaker/MedQA-USMLE-4-options': 500}\n",
            "\n",
            " ì˜ë£Œ ë¶„ì•¼ ë¶„í¬:\n",
            "  â€¢ Medical Knowledge: 500ê°œ\n",
            "\n",
            " ì„ë² ë”© ì •ë³´:\n",
            "  â€¢ ì„ë² ë”© ì°¨ì›: 1536\n",
            "  â€¢ ì„ë² ë”© ëª¨ë¸: text-embedding-3-small\n",
            "\n",
            "================================================================================\n",
            " Phase 1 ë‹¬ì„± ëª©í‘œ:\n",
            "  1. âœ“ MedQA ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì™„ë£Œ\n",
            "  2. âœ“ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ\n",
            "  3. âœ“ ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°œë°œ ì™„ë£Œ\n",
            "  4. âœ“ ì¸í„°ë™í‹°ë¸Œ ì±—ë´‡ í”„ë¡œí† íƒ€ì… ì™„ë£Œ\n",
            "\n",
            " ë‹¤ìŒ ë‹¨ê³„ (Phase 2) ê¶Œì¥ì‚¬í•­:\n",
            "  1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ë²¡í„°) êµ¬í˜„\n",
            "  2. ì˜ë£Œ ìš©ì–´ ë™ì˜ì–´ ë§¤í•‘ ì‹œìŠ¤í…œ ì¶”ê°€\n",
            "  3. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ ê³ ë„í™”\n",
            "  4. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­ ë„ì…\n",
            "  5. ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ (Streamlit/Gradio)\n",
            "\n",
            " ê°œì„  ì•„ì´ë””ì–´:\n",
            "  â€¢ ì˜ë£Œ ë¶„ì•¼ë³„ ì „ë¬¸ ëª¨ë¸ í™œìš©\n",
            "  â€¢ ì‹¤ì‹œê°„ ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™\n",
            "  â€¢ ë‹¤êµ­ì–´ ì§€ì› (í•œì˜ ì˜ë£Œ ìš©ì–´)\n",
            "  â€¢ ì˜ë£Œì˜ìƒ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€\n",
            "  â€¢ í™˜ì ê¸°ë¡ ì—°ë™ ì‹œìŠ¤í…œ\n",
            "\n",
            "================================================================================\n",
            " ì˜ë£Œ RAG ì‹œìŠ¤í…œ Phase 1 ì™„ë£Œ!\n",
            "\n",
            "ğŸ’¾ ì €ì¥ëœ íŒŒì¼:\n",
            "  â€¢ medqa_embeddings.csv (17.7 MB)\n"
          ]
        }
      ],
      "source": [
        "def generate_system_report(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±\n",
        "    \"\"\"\n",
        "    print(\" MedQA ê¸°ë°˜ ì˜ë£Œ RAG ì‹œìŠ¤í…œ - Phase 1 ì™„ë£Œ ë³´ê³ ì„œ\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # ë°ì´í„° í†µê³„\n",
        "    print(\" ë°ì´í„° í†µê³„:\")\n",
        "    print(f\"  â€¢ ì´ ì˜ë£Œ ë¬¸ì„œ ìˆ˜: {len(df):,}ê°œ\")\n",
        "    print(f\"  â€¢ í‰ê·  í† í° ìˆ˜: {df['n_tokens'].mean():.1f}\")\n",
        "    print(f\"  â€¢ ìµœëŒ€ í† í° ìˆ˜: {df['n_tokens'].max():,}\")\n",
        "    print(f\"  â€¢ ë°ì´í„° ì¶œì²˜: {df['source'].value_counts().to_dict()}\")\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ ë¶„í¬\n",
        "    print(\"\\n ì˜ë£Œ ë¶„ì•¼ ë¶„í¬:\")\n",
        "    category_counts = df['category'].value_counts()\n",
        "    for category, count in category_counts.head(10).items():\n",
        "        print(f\"  â€¢ {category}: {count}ê°œ\")\n",
        "\n",
        "    # ì„ë² ë”© ì •ë³´\n",
        "    if len(df) > 0 and 'embeddings' in df.columns:\n",
        "        sample_embedding = df['embeddings'].iloc[0]\n",
        "        if isinstance(sample_embedding, list):\n",
        "            embedding_dim = len(sample_embedding)\n",
        "        else:\n",
        "            embedding_dim = \"Unknown\"\n",
        "        print(f\"\\n ì„ë² ë”© ì •ë³´:\")\n",
        "        print(f\"  â€¢ ì„ë² ë”© ì°¨ì›: {embedding_dim}\")\n",
        "        print(f\"  â€¢ ì„ë² ë”© ëª¨ë¸: {embedding_model}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    # Phase 1 ì„±ê³¼\n",
        "    print(\" Phase 1 ë‹¬ì„± ëª©í‘œ:\")\n",
        "    print(\"  1. âœ“ MedQA ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "    print(\"  2. âœ“ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ\")\n",
        "    print(\"  3. âœ“ ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°œë°œ ì™„ë£Œ\")\n",
        "    print(\"  4. âœ“ ì¸í„°ë™í‹°ë¸Œ ì±—ë´‡ í”„ë¡œí† íƒ€ì… ì™„ë£Œ\")\n",
        "\n",
        "    # ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ\n",
        "    print(\"\\n ë‹¤ìŒ ë‹¨ê³„ (Phase 2) ê¶Œì¥ì‚¬í•­:\")\n",
        "    print(\"  1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ë²¡í„°) êµ¬í˜„\")\n",
        "    print(\"  2. ì˜ë£Œ ìš©ì–´ ë™ì˜ì–´ ë§¤í•‘ ì‹œìŠ¤í…œ ì¶”ê°€\")\n",
        "    print(\"  3. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ ê³ ë„í™”\")\n",
        "    print(\"  4. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­ ë„ì…\")\n",
        "    print(\"  5. ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ (Streamlit/Gradio)\")\n",
        "\n",
        "    print(\"\\n ê°œì„  ì•„ì´ë””ì–´:\")\n",
        "    print(\"  â€¢ ì˜ë£Œ ë¶„ì•¼ë³„ ì „ë¬¸ ëª¨ë¸ í™œìš©\")\n",
        "    print(\"  â€¢ ì‹¤ì‹œê°„ ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™\")\n",
        "    print(\"  â€¢ ë‹¤êµ­ì–´ ì§€ì› (í•œì˜ ì˜ë£Œ ìš©ì–´)\")\n",
        "    print(\"  â€¢ ì˜ë£Œì˜ìƒ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€\")\n",
        "    print(\"  â€¢ í™˜ì ê¸°ë¡ ì—°ë™ ì‹œìŠ¤í…œ\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" ì˜ë£Œ RAG ì‹œìŠ¤í…œ Phase 1 ì™„ë£Œ!\")\n",
        "\n",
        "# ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±\n",
        "generate_system_report(medqa_df)\n",
        "\n",
        "# ë°ì´í„° ì €ì¥ í™•ì¸\n",
        "print(\"\\nğŸ’¾ ì €ì¥ëœ íŒŒì¼:\")\n",
        "if os.path.exists('medqa_embeddings.csv'):\n",
        "    file_size = os.path.getsize('medqa_embeddings.csv') / 1024 / 1024  # MB\n",
        "    print(f\"  â€¢ medqa_embeddings.csv ({file_size:.1f} MB)\")\n",
        "else:\n",
        "    print(\"  â€¢ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨\")"
      ]
    }
  ]
}